{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: Show me the last 5 days of stock prices\n",
      "==================================================\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: sql_db_list_tables\n",
      "Action Input: \"\"\u001b[0m\u001b[38;5;200m\u001b[1;3mconsumption\u001b[0m\u001b[32;1m\u001b[1;3mThought: The 'consumption' table seems relevant for stock price data. Let me check the schema for that table.\n",
      "Action: sql_db_schema\n",
      "Action Input: consumption\u001b[0m\u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE consumption (\n",
      "\t\"Unnamed: 0\" BIGINT, \n",
      "\t\"Date\" TEXT, \n",
      "\t\"Price\" FLOAT, \n",
      "\t\"Close\" FLOAT, \n",
      "\t\"High\" FLOAT, \n",
      "\t\"Low\" FLOAT, \n",
      "\t\"Open\" FLOAT, \n",
      "\t\"Volume\" FLOAT\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from consumption table:\n",
      "Unnamed: 0\tDate\tPrice\tClose\tHigh\tLow\tOpen\tVolume\n",
      "0\t2023-12-18\t194.9350128173828\t195.88999938964844\t196.6300048828125\t194.38999938964844\t196.08999633789065\t55751900.0\n",
      "1\t2023-12-19\t195.9798889160156\t196.94000244140625\t196.9499969482422\t195.88999938964844\t196.16000366210935\t40714100.0\n",
      "2\t2023-12-20\t193.88018798828125\t194.8300018310547\t197.67999267578125\t194.8300018310547\t196.8999938964844\t52242800.0\n",
      "*/\u001b[0m\u001b[32;1m\u001b[1;3mThought: The 'consumption' table contains stock price data with columns for date, open, close, high, low, and volume. To get the last 5 days of prices, I can query this table and filter for the most recent dates.\n",
      "\n",
      "Action: sql_db_query_checker\n",
      "Action Input:\n",
      "SELECT \n",
      "    \"Date\", \n",
      "    \"Open\" AS open_price,\n",
      "    \"Close\" AS close_price,\n",
      "    \"High\" AS high_price, \n",
      "    \"Low\" AS low_price,\n",
      "    \"Volume\" AS volume\n",
      "FROM consumption\n",
      "ORDER BY \"Date\" DESC\n",
      "LIMIT 5;\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mSELECT \n",
      "    \"Date\", \n",
      "    \"Open\" AS open_price,\n",
      "    \"Close\" AS close_price,\n",
      "    \"High\" AS high_price, \n",
      "    \"Low\" AS low_price,\n",
      "    \"Volume\" AS volume\n",
      "FROM consumption\n",
      "ORDER BY \"Date\" DESC\n",
      "LIMIT 5;\u001b[0m\u001b[32;1m\u001b[1;3mThe query looks good, it will select the date, open, close, high, low and volume columns from the consumption table, order the results by date in descending order, and limit to the last 5 rows. This should give the desired last 5 days of stock price data.\n",
      "\n",
      "Action: sql_db_query\n",
      "Action Input:\n",
      "SELECT \n",
      "    \"Date\", \n",
      "    \"Open\" AS open_price,\n",
      "    \"Close\" AS close_price,\n",
      "    \"High\" AS high_price, \n",
      "    \"Low\" AS low_price,\n",
      "    \"Volume\" AS volume  \n",
      "FROM consumption\n",
      "ORDER BY \"Date\" DESC\n",
      "LIMIT 5;\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[('2024-12-16', 247.9900054931641, 251.0399932861328, 251.3800048828125, 247.6499938964844, 51665600.0), ('2024-12-13', 247.82000732421875, 248.1300048828125, 249.2899932861328, 246.2400054931641, 33155300.0), ('2024-12-12', 246.88999938964844, 247.9600067138672, 248.7400054931641, 245.67999267578125, 32777500.0), ('2024-12-11', 247.9600067138672, 246.4900054931641, 250.8000030517578, 246.2599945068359, 45205800.0), ('2024-12-10', 246.88999938964844, 247.77000427246097, 248.2100067138672, 245.33999633789065, 36914800.0)]\u001b[0m\u001b[32;1m\u001b[1;3mThe query successfully retrieved the last 5 days of stock price data from the consumption table, including the date, open price, close price, high price, low price, and trading volume for each day. The results are ordered by date in descending order, so the most recent date is first.\n",
      "\n",
      "Final Answer: The query provided the last 5 days of stock prices, including the open, close, high, low and volume for each day. The most recent trading day was 2024-12-16, where the stock opened at $247.99, reached a high of $251.38, low of $247.65, and closed at $251.04 with volume of 51,665,600 shares traded. The previous 4 days of price data were also returned in descending date order.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Error: cannot access local variable 'sql_query' where it is not associated with a value\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Imports and Basic Setup\n",
    "# Import required libraries for data processing, database operations, language models and environment variables\n",
    "import os\n",
    "from typing import Dict, List, Optional, TypedDict, Literal, Union\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import json\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\n",
    "import sqlite3\n",
    "import re\n",
    "\n",
    "# Load API keys from environment file\n",
    "load_dotenv('api_key.env')\n",
    "\n",
    "# Part 2: Type Definitions and Base Classes\n",
    "# Define types and base classes for query handling and state management\n",
    "\n",
    "# Enum for different types of queries that can be processed\n",
    "class QueryType(Enum):\n",
    "    DIRECT_SQL = \"direct_sql\"  # For simple data retrieval\n",
    "    ANALYSIS = \"analysis\"      # For complex analysis\n",
    "\n",
    "# Class to store query classification results\n",
    "@dataclass\n",
    "class QueryClassification:\n",
    "    type: QueryType\n",
    "    explanation: str\n",
    "    raw_response: str\n",
    "\n",
    "# TypedDict to maintain state during analysis workflow\n",
    "class AnalysisState(TypedDict):\n",
    "    user_query: str\n",
    "    query_classification: Dict\n",
    "    decomposed_questions: List[str]\n",
    "    sql_results: Dict\n",
    "    analysis: str\n",
    "    final_output: Dict\n",
    "    token_usage: Dict\n",
    "    processing_time: float\n",
    "    agent_states: Dict  # Store intermediate states and raw outputs\n",
    "    raw_responses: Dict # Store raw LLM responses\n",
    "\n",
    "# Custom exception for configuration related errors\n",
    "class ConfigError(Exception):\n",
    "    \"\"\"Custom exception for configuration errors\"\"\"\n",
    "    pass\n",
    "\n",
    "# Configuration class to store and validate settings\n",
    "@dataclass\n",
    "class Config:\n",
    "    db_path: str = \"apple_last_year_data.csv\"\n",
    "    sqlite_path: str = \"sqlite:///consumption.db\"\n",
    "    model_name: str = \"claude-3-sonnet-20240229\"\n",
    "    \n",
    "    @property\n",
    "    def api_key(self) -> str:\n",
    "        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ConfigError(\"ANTHROPIC_API_KEY not found in api_key.env file\")\n",
    "        return api_key\n",
    "\n",
    "# Part 3: Prompt Templates\n",
    "# Define system prompts for different stages of analysis\n",
    "\n",
    "# Prompt for classifying incoming queries\n",
    "QUERY_CLASSIFIER_PROMPT = \"\"\"You are a query classifier that determines if a stock market question needs complex analysis or can be answered with a direct SQL query.\n",
    "\n",
    "Example 1:\n",
    "Question: \"Show me the last 5 days of stock prices\"\n",
    "Classification: direct_sql\n",
    "Explanation: This is a straightforward data retrieval request.\n",
    "\n",
    "Example 2:\n",
    "Question: \"What are the emerging trends in trading volume and their impact on price?\"\n",
    "Classification: analysis\n",
    "Explanation: This requires complex analysis of relationships and patterns.\n",
    "\n",
    "Respond in JSON format:\n",
    "{\n",
    "    \"type\": \"direct_sql\" or \"analysis\",\n",
    "    \"explanation\": \"brief explanation of classification\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for SQL agent to handle database queries\n",
    "SQL_AGENT_PROMPT = \"\"\"You are an expert financial database analyst. Your task is to:\n",
    "1. Analyze stock market queries\n",
    "2. Create appropriate SQL queries\n",
    "3. Provide clear results\n",
    "\n",
    "Example 1:\n",
    "User: \"What's the stock's performance last week?\"\n",
    "Thought: Need to analyze daily price changes and volume for the past week\n",
    "SQL:\n",
    "SELECT \n",
    "    date,\n",
    "    ROUND(open, 2) as open_price,\n",
    "    ROUND(close, 2) as close_price,\n",
    "    ROUND(((close - open) / open * 100), 2) as daily_return,\n",
    "    ROUND(high, 2) as high,\n",
    "    ROUND(low, 2) as low,\n",
    "    volume\n",
    "FROM consumption\n",
    "WHERE date >= date('now', '-7 days')\n",
    "ORDER BY date DESC;\n",
    "\n",
    "Example 2:\n",
    "User: \"Find volatile trading days\"\n",
    "Thought: Looking for days with large price ranges and high volume\n",
    "SQL:\n",
    "WITH metrics AS (\n",
    "    SELECT AVG(volume) as avg_vol,\n",
    "           AVG((high - low) / open * 100) as avg_range\n",
    "    FROM consumption\n",
    ")\n",
    "SELECT \n",
    "    date,\n",
    "    ROUND(open, 2) as open_price,\n",
    "    ROUND(close, 2) as close_price,\n",
    "    ROUND(((high - low) / open * 100), 2) as price_range_pct,\n",
    "    volume,\n",
    "    ROUND(volume / avg_vol, 2) as vol_ratio\n",
    "FROM consumption, metrics\n",
    "WHERE (high - low) / open * 100 > avg_range\n",
    "AND volume > avg_vol\n",
    "ORDER BY price_range_pct DESC\n",
    "LIMIT 5;\n",
    "\n",
    "Your responses should include:\n",
    "1. Thought process\n",
    "2. SQL query\n",
    "3. Result interpretation\"\"\"\n",
    "\n",
    "# Prompt for financial analysis of SQL results\n",
    "ANALYST_PROMPT = \"\"\"You are an expert financial analyst. Analyze the provided SQL results and provide insights.\n",
    "\n",
    "Focus on:\n",
    "1. Price trends and patterns\n",
    "2. Volume analysis\n",
    "3. Technical indicators\n",
    "4. Risk assessment\n",
    "5. Notable patterns\n",
    "\n",
    "Example Analysis Structure:\n",
    "1. Key Findings\n",
    "   - Main price trends\n",
    "   - Volume patterns\n",
    "   - Notable events\n",
    "\n",
    "2. Technical Analysis\n",
    "   - Support/resistance levels\n",
    "   - Pattern recognition\n",
    "   - Momentum indicators\n",
    "\n",
    "3. Risk Assessment\n",
    "   - Volatility measures\n",
    "   - Liquidity analysis\n",
    "   - Risk factors\n",
    "\n",
    "4. Recommendations\n",
    "   - Key levels to watch\n",
    "   - Risk considerations\n",
    "   - Potential scenarios\n",
    "\n",
    "Be specific and data-driven in your analysis.\"\"\"\n",
    "\n",
    "# Part 4: Main StockAnalyzer Class\n",
    "# Core class that orchestrates the entire analysis process\n",
    "class StockAnalyzer:\n",
    "    def __init__(self, config: Config):\n",
    "        \"\"\"Initialize analyzer with configuration and setup components\"\"\"\n",
    "        self.config = config\n",
    "        self.db = self._init_database()\n",
    "        self.llm = self._init_llm()\n",
    "        self.sql_agent = self._setup_sql_agent()\n",
    "        self.workflow = self._create_workflow()\n",
    "        self.token_usage = {\"prompt_tokens\": 0, \"completion_tokens\": 0}\n",
    "        self.anthropic_client = Anthropic(api_key=config.api_key)\n",
    "        self.agent_states = {}  # Store intermediate states\n",
    "        self.raw_responses = {} # Store raw responses\n",
    "        self.conn = sqlite3.connect('consumption.db')\n",
    "\n",
    "    def _init_database(self) -> SQLDatabase:\n",
    "        \"\"\"Initialize SQLite database from CSV data\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(self.config.db_path)\n",
    "            df.to_sql('consumption', 'sqlite:///consumption.db', index=False, if_exists='replace')\n",
    "            return SQLDatabase.from_uri(self.config.sqlite_path)\n",
    "        except Exception as e:\n",
    "            raise ConfigError(f\"Database initialization failed: {str(e)}\")\n",
    "\n",
    "    def _init_llm(self) -> ChatAnthropic:\n",
    "        \"\"\"Initialize the language model with specified configuration\"\"\"\n",
    "        return ChatAnthropic(\n",
    "            model=self.config.model_name,\n",
    "            temperature=0,\n",
    "            api_key=self.config.api_key\n",
    "        )\n",
    "\n",
    "    def _setup_sql_agent(self):\n",
    "        \"\"\"Setup SQL agent with database toolkit and language model\"\"\"\n",
    "        toolkit = SQLDatabaseToolkit(db=self.db, llm=self.llm)\n",
    "        return create_sql_agent(\n",
    "            llm=self.llm,\n",
    "            toolkit=toolkit,\n",
    "            agent_type=\"zero-shot-react-description\",\n",
    "            verbose=True,\n",
    "            prefix=SQL_AGENT_PROMPT\n",
    "        )\n",
    "\n",
    "    def _update_token_usage(self, response):\n",
    "        \"\"\"Track token usage from model responses\"\"\"\n",
    "        try:\n",
    "            if hasattr(response, '_raw_response') and 'usage' in response._raw_response:\n",
    "                usage = response._raw_response['usage']\n",
    "                self.token_usage[\"prompt_tokens\"] += usage.get('input_tokens', 0)\n",
    "                self.token_usage[\"completion_tokens\"] += usage.get('output_tokens', 0)\n",
    "            elif isinstance(response, dict) and 'usage' in response:\n",
    "                usage = response['usage']\n",
    "                self.token_usage[\"prompt_tokens\"] += usage.get('input_tokens', 0)\n",
    "                self.token_usage[\"completion_tokens\"] += usage.get('output_tokens', 0)\n",
    "            elif hasattr(response, 'usage'):\n",
    "                usage = response.usage\n",
    "                self.token_usage[\"prompt_tokens\"] += usage.input_tokens if hasattr(usage, 'input_tokens') else 0\n",
    "                self.token_usage[\"completion_tokens\"] += usage.output_tokens if hasattr(usage, 'output_tokens') else 0\n",
    "            else:\n",
    "                # Make a direct API call to get token count\n",
    "                message = response.content if hasattr(response, 'content') else str(response)\n",
    "                result = self.anthropic_client.messages.create(\n",
    "                    model=self.config.model_name,\n",
    "                    messages=[{\"role\": \"user\", \"content\": message}],\n",
    "                    max_tokens=1\n",
    "                )\n",
    "                if hasattr(result, 'usage'):\n",
    "                    self.token_usage[\"prompt_tokens\"] += result.usage.input_tokens\n",
    "                    self.token_usage[\"completion_tokens\"] += result.usage.output_tokens\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating token usage: {str(e)}\")\n",
    "\n",
    "    def _classify_query(self, query: str) -> QueryClassification:\n",
    "        \"\"\"Determine if query needs complex analysis or direct SQL\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = self.llm.invoke([\n",
    "                SystemMessage(content=QUERY_CLASSIFIER_PROMPT),\n",
    "                HumanMessage(content=f\"Classify this question: {query}\")\n",
    "            ])\n",
    "            \n",
    "            self._update_token_usage(response)\n",
    "            classification = json.loads(response.content)\n",
    "            \n",
    "            # Store raw response\n",
    "            self.raw_responses['classification'] = response.content\n",
    "            \n",
    "            return QueryClassification(\n",
    "                type=QueryType(classification['type']),\n",
    "                explanation=classification['explanation'],\n",
    "                raw_response=response.content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return QueryClassification(\n",
    "                type=QueryType.ANALYSIS,\n",
    "                explanation=\"Classification failed, defaulting to analysis\",\n",
    "                raw_response=str(e)\n",
    "            )\n",
    "\n",
    "    def _direct_sql_query(self, query: str) -> Dict:\n",
    "        \"\"\"Process simple queries that only need SQL execution\"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            result = self.sql_agent.invoke({\"input\": query})\n",
    "            self._update_token_usage(result)\n",
    "            \n",
    "            # Store agent state\n",
    "            self.agent_states['direct_sql'] = result\n",
    "            \n",
    "            thought = self._extract_thought(result['output'])\n",
    "            sql = self._extract_sql(result['output'])\n",
    "            \n",
    "            # Execute SQL query and get results\n",
    "            try:\n",
    "                # Extract SQL query from agent output\n",
    "                sql_query = sql if sql else result['output'].split('SELECT')[1].split('Final Answer')[0].strip()\n",
    "                if not sql_query.lower().startswith('select'):\n",
    "                    sql_query = 'SELECT ' + sql_query\n",
    "                \n",
    "                # Clean up SQL query by removing any text after the first semicolon\n",
    "                sql_query = sql_query.split(';')[0] + ';'\n",
    "                \n",
    "                # Execute query and get results\n",
    "                df = pd.read_sql_query(sql_query, self.conn)\n",
    "                formatted_results = df.to_dict('records')\n",
    "            except Exception as e:\n",
    "                formatted_results = f\"Error executing SQL: {str(e)}\"\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            output_data = {\n",
    "                \"query_type\": \"direct_sql\",\n",
    "                \"user_query\": query,\n",
    "                \"thought_process\": thought if thought else \"No thought process provided\",\n",
    "                \"sql_query\": sql if sql else sql_query,\n",
    "                \"results\": formatted_results,\n",
    "                \"raw_agent_output\": result['output'],\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "                \"token_usage\": self.token_usage,\n",
    "                \"processing_time\": processing_time,\n",
    "                \"agent_state\": result\n",
    "            }\n",
    "            \n",
    "            # Save to query-specific output file\n",
    "            filename = f\"{query[:50].replace(' ', '_').lower()}_analysis.json\"\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(output_data, f, indent=2)\n",
    "                \n",
    "            return output_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"query\": query}\n",
    "\n",
    "    def _decompose_question(self, state: Dict) -> Dict:\n",
    "        \"\"\"Break down complex questions into simpler sub-questions\"\"\"\n",
    "        response = self.llm.invoke([\n",
    "            SystemMessage(content=\"Break down this stock analysis question into specific sub-questions that can be answered with SQL queries:\"),\n",
    "            HumanMessage(content=state['user_query'])\n",
    "        ])\n",
    "        \n",
    "        self._update_token_usage(response)\n",
    "        \n",
    "        # Store raw response\n",
    "        self.raw_responses['decomposition'] = response.content\n",
    "        \n",
    "        questions = [\n",
    "            q.strip().split(\". \", 1)[1] if \". \" in q else q.strip()\n",
    "            for q in response.content.split(\"\\n\")\n",
    "            if q.strip() and q[0].isdigit()\n",
    "        ]\n",
    "        \n",
    "        state['decomposed_questions'] = questions\n",
    "        state['agent_states']['decomposition'] = {\n",
    "            'raw_response': response.content,\n",
    "            'parsed_questions': questions\n",
    "        }\n",
    "        return state\n",
    "\n",
    "    def _run_sql_analysis(self, state: Dict) -> Dict:\n",
    "        \"\"\"Execute SQL queries for each sub-question\"\"\"\n",
    "        results = {}\n",
    "        agent_states = {}\n",
    "        \n",
    "        for i, question in enumerate(state[\"decomposed_questions\"], 1):\n",
    "            try:\n",
    "                result = self.sql_agent.invoke({\"input\": question})\n",
    "                self._update_token_usage(result)\n",
    "                \n",
    "                # Store agent state\n",
    "                agent_states[f\"question_{i}\"] = result\n",
    "                \n",
    "                thought = self._extract_thought(result['output'])\n",
    "                sql = self._extract_sql(result['output'])\n",
    "                \n",
    "                # Execute SQL query and get results\n",
    "                try:\n",
    "                    # Clean up SQL query by removing any text after the first semicolon\n",
    "                    sql = sql.split(';')[0] + ';'\n",
    "                    \n",
    "                    df = pd.read_sql_query(sql, self.conn)\n",
    "                    parsed_result = df.to_dict('records')\n",
    "                except Exception as e:\n",
    "                    parsed_result = f\"Error executing SQL: {str(e)}\"\n",
    "                \n",
    "                results[f\"question_{i}\"] = {\n",
    "                    \"question\": question,\n",
    "                    \"thought\": thought if thought else \"No thought process provided\",\n",
    "                    \"sql\": sql if sql else \"No SQL query provided\",\n",
    "                    \"result\": parsed_result,\n",
    "                    \"raw_output\": result['output']\n",
    "                }\n",
    "                    \n",
    "            except Exception as e:\n",
    "                results[f\"question_{i}\"] = {\n",
    "                    \"error\": str(e),\n",
    "                    \"question\": question\n",
    "                }\n",
    "        \n",
    "        state['sql_results'] = results\n",
    "        state['agent_states']['sql_analysis'] = agent_states\n",
    "        return state\n",
    "\n",
    "    def _analyze_results(self, state: Dict) -> Dict:\n",
    "        \"\"\"Generate comprehensive analysis from SQL results\"\"\"\n",
    "        results_context = json.dumps(state[\"sql_results\"], indent=2)\n",
    "        response = self.llm.invoke([\n",
    "            SystemMessage(content=ANALYST_PROMPT),\n",
    "            HumanMessage(content=f\"\"\"\n",
    "            Original Question: {state['user_query']}\n",
    "            \n",
    "            Analysis Results:\n",
    "            {results_context}\n",
    "            \n",
    "            Provide a comprehensive analysis.\"\"\")\n",
    "        ])\n",
    "        \n",
    "        self._update_token_usage(response)\n",
    "        \n",
    "        # Store raw response\n",
    "        self.raw_responses['analysis'] = response.content\n",
    "        \n",
    "        state['analysis'] = response.content\n",
    "        state['agent_states']['analysis'] = {\n",
    "            'raw_response': response.content\n",
    "        }\n",
    "            \n",
    "        return state\n",
    "\n",
    "    def _format_output(self, state: Dict) -> Dict:\n",
    "        \"\"\"Format final analysis results\"\"\"\n",
    "        state['final_output'] = {\n",
    "            \"query_type\": \"analysis\",\n",
    "            \"user_query\": state[\"user_query\"],\n",
    "            \"query_classification\": state.get(\"query_classification\", {}),\n",
    "            \"sub_questions\": state[\"decomposed_questions\"],\n",
    "            \"sql_analysis\": state[\"sql_results\"],\n",
    "            \"expert_analysis\": state[\"analysis\"],\n",
    "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"token_usage\": self.token_usage,\n",
    "            \"processing_time\": state.get(\"processing_time\", 0),\n",
    "            \"agent_states\": self.agent_states,\n",
    "            \"raw_responses\": self.raw_responses\n",
    "        }\n",
    "        \n",
    "        # Save complete analysis output with query-specific filename\n",
    "        filename = f\"{state['user_query'][:50].replace(' ', '_').lower()}_analysis.json\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(state['final_output'], f, indent=2)\n",
    "            \n",
    "        return state\n",
    "\n",
    "    def _extract_thought(self, text: str) -> str:\n",
    "        \"\"\"Extract thought process from agent response\"\"\"\n",
    "        if \"Thought:\" in text:\n",
    "            return text.split(\"Thought:\")[1].split(\"SQL\")[0].strip()\n",
    "        return \"\"\n",
    "\n",
    "    def _extract_sql(self, text: str) -> str:\n",
    "        \"\"\"Extract SQL query from agent response\"\"\"\n",
    "        if \"SQL:\" in text:\n",
    "            sql_part = text.split(\"SQL:\")[1]\n",
    "            if \"SQLResult:\" in sql_part:\n",
    "                return sql_part.split(\"SQLResult:\")[0].strip()\n",
    "            if \"Final Answer:\" in sql_part:\n",
    "                return sql_part.split(\"Final Answer:\")[0].strip()\n",
    "            return sql_part.strip()\n",
    "        return \"\"\n",
    "\n",
    "    def _extract_result(self, text: str) -> str:\n",
    "        \"\"\"Extract results from agent response\"\"\"\n",
    "        if \"SQLResult:\" in text:\n",
    "            return text.split(\"SQLResult:\")[1].strip()\n",
    "        return \"\"\n",
    "\n",
    "    def _create_workflow(self) -> StateGraph:\n",
    "        \"\"\"Create analysis workflow graph\"\"\"\n",
    "        workflow = StateGraph(AnalysisState)\n",
    "        \n",
    "        workflow.add_node(\"decompose\", self._decompose_question)\n",
    "        workflow.add_node(\"sql_analysis\", self._run_sql_analysis)\n",
    "        workflow.add_node(\"analyze\", self._analyze_results)\n",
    "        workflow.add_node(\"format\", self._format_output)\n",
    "        \n",
    "        workflow.add_edge(START, \"decompose\")\n",
    "        workflow.add_edge(\"decompose\", \"sql_analysis\")\n",
    "        workflow.add_edge(\"sql_analysis\", \"analyze\")\n",
    "        workflow.add_edge(\"analyze\", \"format\")\n",
    "        workflow.add_edge(\"format\", END)\n",
    "        \n",
    "        return workflow.compile()\n",
    "\n",
    "    def analyze(self, query: str) -> Dict:\n",
    "        \"\"\"Main method to process queries and generate analysis\"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # Reset storages for new analysis\n",
    "            self.token_usage = {\"prompt_tokens\": 0, \"completion_tokens\": 0}\n",
    "            self.agent_states = {}\n",
    "            self.raw_responses = {}\n",
    "            \n",
    "            # First, classify the query\n",
    "            classification = self._classify_query(query)\n",
    "            \n",
    "            # For direct SQL queries, use simplified processing\n",
    "            if classification.type == QueryType.DIRECT_SQL:\n",
    "                return self._direct_sql_query(query)\n",
    "            \n",
    "            # For analysis queries, use the full workflow\n",
    "            initial_state = {\n",
    "                \"user_query\": query,\n",
    "                \"query_classification\": {\n",
    "                    \"type\": classification.type.value,\n",
    "                    \"explanation\": classification.explanation,\n",
    "                    \"raw_response\": classification.raw_response\n",
    "                },\n",
    "                \"decomposed_questions\": [],\n",
    "                \"sql_results\": {},\n",
    "                \"analysis\": \"\",\n",
    "                \"final_output\": {},\n",
    "                \"token_usage\": self.token_usage,\n",
    "                \"processing_time\": 0,\n",
    "                \"agent_states\": {},\n",
    "                \"raw_responses\": {}\n",
    "            }\n",
    "            \n",
    "            final_state = self.workflow.invoke(initial_state)\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            final_state[\"processing_time\"] = processing_time\n",
    "            final_state[\"token_usage\"] = self.token_usage\n",
    "            \n",
    "            # Update final output\n",
    "            final_state[\"final_output\"][\"processing_time\"] = processing_time\n",
    "            final_state[\"final_output\"][\"token_usage\"] = self.token_usage\n",
    "            final_state[\"final_output\"][\"agent_states\"] = self.agent_states\n",
    "            final_state[\"final_output\"][\"raw_responses\"] = self.raw_responses\n",
    "            \n",
    "            return final_state[\"final_output\"]\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"query\": query}\n",
    "        finally:\n",
    "            self.conn.close()\n",
    "\n",
    "# Part 5: Helper Functions\n",
    "def format_output(results: Dict) -> str:\n",
    "    \"\"\"Format analysis results in a readable string format\"\"\"\n",
    "    output = []\n",
    "    output.append(\"=== Stock Analysis Results ===\")\n",
    "    output.append(f\"\\nQuery: {results.get('user_query', 'N/A')}\")\n",
    "    \n",
    "    # Add performance metrics\n",
    "    output.append(f\"\\nProcessing Time: {results.get('processing_time', 0):.2f} seconds\")\n",
    "    token_usage = results.get('token_usage', {})\n",
    "    output.append(f\"Token Usage:\")\n",
    "    output.append(f\"  Prompt Tokens: {token_usage.get('prompt_tokens', 0)}\")\n",
    "    output.append(f\"  Completion Tokens: {token_usage.get('completion_tokens', 0)}\")\n",
    "    output.append(f\"  Total Tokens: {token_usage.get('prompt_tokens', 0) + token_usage.get('completion_tokens', 0)}\")\n",
    "    \n",
    "    if \"error\" in results:\n",
    "        output.append(f\"\\nError: {results['error']}\")\n",
    "        return \"\\n\".join(output)\n",
    "    \n",
    "    if results.get('query_type') == 'direct_sql':\n",
    "        output.append(f\"\\nThought Process: {results.get('thought_process', 'N/A')}\")\n",
    "        output.append(f\"\\nSQL Query: {results.get('sql_query', 'N/A')}\")\n",
    "        output.append(\"\\nResults:\")\n",
    "        if isinstance(results.get('results'), list):\n",
    "            df = pd.DataFrame(results['results'])\n",
    "            output.append(str(df))\n",
    "        else:\n",
    "            output.append(str(results.get('results', 'No results available')))\n",
    "    else:\n",
    "        output.append(\"\\nSub-Questions:\")\n",
    "        for i, q in enumerate(results.get('sub_questions', []), 1):\n",
    "            output.append(f\"{i}. {q}\")\n",
    "        \n",
    "        output.append(\"\\nSQL Analysis:\")\n",
    "        for key, data in results.get('sql_analysis', {}).items():\n",
    "            output.append(f\"\\nQuestion: {data.get('question', 'N/A')}\")\n",
    "            if 'error' not in data:\n",
    "                output.append(f\"Thought Process: {data.get('thought', 'N/A')}\")\n",
    "                output.append(f\"SQL Query: {data.get('sql', 'N/A')}\")\n",
    "                try:\n",
    "                    if isinstance(data.get('result'), (list, dict)):\n",
    "                        df = pd.DataFrame(data['result'])\n",
    "                        output.append(str(df))\n",
    "                    else:\n",
    "                        output.append(f\"Results: {data.get('result', 'No results available')}\")\n",
    "                except:\n",
    "                    output.append(f\"Results: {data.get('result', 'No results available')}\")\n",
    "            else:\n",
    "                output.append(f\"Error: {data['error']}\")\n",
    "        \n",
    "        output.append(\"\\nExpert Analysis:\")\n",
    "        output.append(results.get('expert_analysis', 'No analysis available'))\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "def analyze_stock_query(query: str) -> str:\n",
    "    \"\"\"Main function to handle stock analysis queries and return formatted results\"\"\"\n",
    "    try:\n",
    "        # Initialize analyzer\n",
    "        config = Config()\n",
    "        analyzer = StockAnalyzer(config)\n",
    "        \n",
    "        # Run analysis\n",
    "        results = analyzer.analyze(query)\n",
    "        \n",
    "        if results and \"error\" not in results:\n",
    "            formatted_output = format_output(results)\n",
    "            filename = f\"{query[:50].replace(' ', '_').lower()}_analysis.json\"\n",
    "            return formatted_output + f\"\\n\\nDetailed results saved to {filename}\"\n",
    "        else:\n",
    "            return f\"Error: {results.get('error', 'Unknown error occurred')}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error during analysis: {str(e)}\"\n",
    "\n",
    "# Part 6: Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Example queries for testing\n",
    "    test_queries = [\n",
    "        \"Show me the last 5 days of stock prices\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nProcessing: {query}\")\n",
    "        print(\"=\" * 50)\n",
    "        result = analyze_stock_query(query)\n",
    "        print(result)\n",
    "        print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Displaying detailed analysis for: Show me the last 5 days of stock prices\n",
      "\n",
      "=== DETAILED ANALYSIS REPORT ===\n",
      "\n",
      "\n",
      "Query Type:direct_sql\n",
      "\n",
      "User Query:Show me the last 5 days of stock prices\n",
      "\n",
      "Thought Process:No thought process provided\n",
      "\n",
      "Sql Query:No SQL query provided\n",
      "\n",
      "Results:No results available\n",
      "\n",
      "Raw Agent Output:The query provided the last 5 days of stock prices, including the open, close, high, low and volume for each day. The most recent trading day was 2024-12-16, where the stock opened at $247.99, reached a high of $251.38, low of $247.65, and closed at $251.04 with volume of 51,665,600 shares traded. The previous 4 days of price data were also returned in descending date order.\n",
      "\n",
      "Timestamp:2024-12-22T19:38:26.497416\n",
      "\n",
      "Token Usage:\n",
      "Prompt Tokens:\n",
      "195\n",
      "Completion Tokens:\n",
      "2\n",
      "\n",
      "Processing Time:16.305702924728394\n",
      "\n",
      "Agent State:\n",
      "Input:\n",
      "Show me the last 5 days of stock prices\n",
      "Output:\n",
      "The query provided the last 5 days of stock prices, including the open, close, high, low and volume for each day. The most recent trading day was 2024-12-16, where the stock opened at $247.99, reached a high of $251.38, low of $247.65, and closed at $251.04 with volume of 51,665,600 shares traded. The previous 4 days of price data were also returned in descending date order.\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def display_json_details(query: str) -> None:\n",
    "    \"\"\"Display detailed JSON analysis results in a readable format\"\"\"\n",
    "    try:\n",
    "        # Generate filename from query\n",
    "        filename = f\"{query[:50].replace(' ', '_').lower()}_analysis.json\"\n",
    "        \n",
    "        # Read and parse JSON file\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        print(\"\\n=== DETAILED ANALYSIS REPORT ===\\n\")\n",
    "\n",
    "        def format_value(value, indent=0):\n",
    "            \"\"\"Recursively format JSON values with proper indentation\"\"\"\n",
    "            indent_str = \"    \" * indent\n",
    "            \n",
    "            if isinstance(value, dict):\n",
    "                print()\n",
    "                for k, v in value.items():\n",
    "                    key_str = k.replace('_', ' ').title()\n",
    "                    print(f\"{indent_str}{key_str}:\")\n",
    "                    format_value(v, indent + 1)\n",
    "\n",
    "            elif isinstance(value, list):\n",
    "                print()\n",
    "                for item in value:\n",
    "                    print(f\"{indent_str}•\", end=' ')\n",
    "                    format_value(item, indent + 1)\n",
    "\n",
    "            elif isinstance(value, (int, float)):\n",
    "                print(f\"{value:,}\")\n",
    "\n",
    "            elif isinstance(value, bool):\n",
    "                print(str(value))\n",
    "\n",
    "            elif value is None:\n",
    "                print(\"None\")\n",
    "\n",
    "            else:\n",
    "                # Handle string values\n",
    "                print(str(value).strip())\n",
    "\n",
    "        # Process each top-level key\n",
    "        for key, value in data.items():\n",
    "            print(f\"\\n{key.replace('_', ' ').title()}:\", end='')\n",
    "            format_value(value)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nError: Analysis file '{filename}' not found\\n\")\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"\\nError: Unable to parse JSON from '{filename}'\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError displaying JSON details: {str(e)}\\n\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    for query in test_queries:\n",
    "        print(f\"\\nDisplaying detailed analysis for: {query}\")\n",
    "        display_json_details(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
