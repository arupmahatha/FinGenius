{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: sql_db_list_tables\n",
      "Action Input: \n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mconsumption\u001b[0m\u001b[32;1m\u001b[1;3mThought: The consumption table seems relevant to find date information, so I should query its schema.\n",
      "Action: sql_db_schema\n",
      "Action Input: consumption\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE consumption (\n",
      "\t\"Unnamed: 0\" INTEGER, \n",
      "\t\"Date\" TEXT, \n",
      "\t\"Price\" REAL, \n",
      "\t\"Close\" REAL, \n",
      "\t\"High\" REAL, \n",
      "\t\"Low\" REAL, \n",
      "\t\"Open\" REAL, \n",
      "\t\"Volume\" REAL\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from consumption table:\n",
      "Unnamed: 0\tDate\tPrice\tClose\tHigh\tLow\tOpen\tVolume\n",
      "0\t2023-12-18\t194.9350128173828\t195.88999938964844\t196.6300048828125\t194.38999938964844\t196.08999633789065\t55751900.0\n",
      "1\t2023-12-19\t195.9798889160156\t196.94000244140625\t196.9499969482422\t195.88999938964844\t196.16000366210935\t40714100.0\n",
      "2\t2023-12-20\t193.88018798828125\t194.8300018310547\t197.67999267578125\t194.8300018310547\t196.8999938964844\t52242800.0\n",
      "*/\u001b[0m\u001b[32;1m\u001b[1;3mThought: The \"Date\" column in the consumption table looks like it contains date information. To find the most recent date, I can query that column, order by it descending, and limit to 1 row.\n",
      "Action: sql_db_query_checker\n",
      "Action Input: SELECT \"Date\" FROM consumption ORDER BY \"Date\" DESC LIMIT 1;\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mSELECT \"Date\" FROM consumption ORDER BY \"Date\" DESC LIMIT 1;\u001b[0m\u001b[32;1m\u001b[1;3mThe query looks good, so I can execute it to get the most recent date.\n",
      "\n",
      "Action: sql_db_query\n",
      "Action Input: SELECT \"Date\" FROM consumption ORDER BY \"Date\" DESC LIMIT 1;\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[('2024-12-16',)]\u001b[0mAn error occurred during analysis: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `The most recent date in the data is 2024-12-16.`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict, TypedDict, Annotated\n",
    "from dataclasses import dataclass\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain.agents import AgentExecutor, create_sql_agent\n",
    "from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "ANTHROPIC_API_KEY = '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'\n",
    "\n",
    "@dataclass\n",
    "class AnalysisQuestion:\n",
    "    question: str\n",
    "    sql_query: str = \"\"\n",
    "    answer: str = \"\"\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: List[tuple]\n",
    "    questions: List[AnalysisQuestion]\n",
    "    final_analysis: str\n",
    "\n",
    "QUESTION_DECOMPOSER_PROMPT = \"\"\"You are an expert at breaking down complex stock analysis questions into specific sub-questions that can be answered using SQL queries.\n",
    "\n",
    "Available data columns: date, open, high, low, close, volume\n",
    "\n",
    "Break down the user's question into specific analytical questions that can be answered with SQL queries.\n",
    "Your response should be in this format:\n",
    "1. First specific question to answer\n",
    "2. Second specific question to answer\n",
    "...\n",
    "\n",
    "Focus on questions that can be answered with the available data columns. Avoid questions about company fundamentals or external factors.\"\"\"\n",
    "\n",
    "ANALYSIS_SYNTHESIZER_PROMPT = \"\"\"You are an expert financial analyst who synthesizes data-driven insights into clear, actionable analysis.\n",
    "\n",
    "Using the answers to our analytical sub-questions, provide a comprehensive analysis that:\n",
    "1. Directly answers the user's original question\n",
    "2. Supports conclusions with specific data points\n",
    "3. Provides context and implications\n",
    "4. Highlights any important caveats or limitations\n",
    "\n",
    "Be concise but thorough. Use actual numbers from the data to support your analysis.\"\"\"\n",
    "\n",
    "def init_sql_agent():\n",
    "    db = SQLDatabase.from_uri(\"sqlite:///consumption.db\")\n",
    "    llm = ChatAnthropic(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        temperature=0,\n",
    "        api_key=ANTHROPIC_API_KEY\n",
    "    )\n",
    "    toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "    return create_sql_agent(\n",
    "        llm=llm,\n",
    "        toolkit=toolkit,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "def decompose_question(state: State):\n",
    "    llm = ChatAnthropic(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        temperature=0,\n",
    "        api_key=ANTHROPIC_API_KEY\n",
    "    )\n",
    "    \n",
    "    original_question = state[\"messages\"][0][1]\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=QUESTION_DECOMPOSER_PROMPT),\n",
    "        HumanMessage(content=original_question)\n",
    "    ])\n",
    "    \n",
    "    # Parse the numbered list response into individual questions\n",
    "    sub_questions = [\n",
    "        AnalysisQuestion(question=q.strip().split(\". \", 1)[1])\n",
    "        for q in response.content.split(\"\\n\")\n",
    "        if q.strip() and q[0].isdigit()\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"questions\": sub_questions,\n",
    "        \"final_analysis\": \"\"\n",
    "    }\n",
    "\n",
    "def get_sql_answers(state: State):\n",
    "    agent = init_sql_agent()\n",
    "    updated_questions = []\n",
    "    \n",
    "    for question in state[\"questions\"]:\n",
    "        result = agent.invoke({\"input\": question.question})\n",
    "        question.answer = result[\"output\"]\n",
    "        updated_questions.append(question)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"questions\": updated_questions,\n",
    "        \"final_analysis\": \"\"\n",
    "    }\n",
    "\n",
    "def synthesize_analysis(state: State):\n",
    "    llm = ChatAnthropic(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        temperature=0,\n",
    "        api_key=ANTHROPIC_API_KEY\n",
    "    )\n",
    "    \n",
    "    # Format sub-questions and their answers\n",
    "    analysis_context = \"\\n\\n\".join([\n",
    "        f\"Question: {q.question}\\nAnswer: {q.answer}\"\n",
    "        for q in state[\"questions\"]\n",
    "    ])\n",
    "    \n",
    "    original_question = state[\"messages\"][0][1]\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=ANALYSIS_SYNTHESIZER_PROMPT),\n",
    "        HumanMessage(content=f\"\"\"Original Question: {original_question}\n",
    "\n",
    "Analysis Components:\n",
    "{analysis_context}\n",
    "\n",
    "Please provide a comprehensive analysis that answers the original question.\"\"\")\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"questions\": state[\"questions\"],\n",
    "        \"final_analysis\": response.content\n",
    "    }\n",
    "\n",
    "def create_analysis_workflow():\n",
    "    workflow = StateGraph(State)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"decompose\", decompose_question)\n",
    "    workflow.add_node(\"analyze\", get_sql_answers)\n",
    "    workflow.add_node(\"synthesize\", synthesize_analysis)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(START, \"decompose\")\n",
    "    workflow.add_edge(\"decompose\", \"analyze\")\n",
    "    workflow.add_edge(\"analyze\", \"synthesize\")\n",
    "    workflow.add_edge(\"synthesize\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "def analyze_stock(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze a stock based on a user question using question decomposition and SQL analysis.\n",
    "    \n",
    "    Args:\n",
    "        question (str): User's analysis question\n",
    "        \n",
    "    Returns:\n",
    "        str: Comprehensive analysis based on the data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        workflow = create_analysis_workflow()\n",
    "        \n",
    "        final_state = None\n",
    "        for state in workflow.stream({\n",
    "            \"messages\": [(\"user\", question)],\n",
    "            \"questions\": [],\n",
    "            \"final_analysis\": \"\"\n",
    "        }):\n",
    "            final_state = state\n",
    "            \n",
    "        return final_state[\"final_analysis\"] if final_state else \"Analysis failed to complete.\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred during analysis: {str(e)}\"\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"Show me the last 10 days price of the stock as a json\"\n",
    "    result = analyze_stock(question)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
