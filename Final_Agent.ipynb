{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_anthropic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatAnthropic\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_sql_agent\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoolkit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SQLDatabaseToolkit\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql_database\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SQLDatabase\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/agents/__init__.py:56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversational\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationalAgent\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversational_chat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationalChatAgent\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitialize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initialize_agent\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson_chat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_json_chat_agent\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_agent\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/agents/initialize.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgentExecutor\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgentType\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AGENT_TO_CLASS, load_agent\n\u001b[1;32m     16\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     message\u001b[38;5;241m=\u001b[39mAGENT_DEPRECATION_WARNING,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     31\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AgentExecutor:\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load an agent executor given tools and LLM.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m        ValueError: If both `agent` and `agent_path` are None.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/agents/loading.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseMultiActionAgent, BaseSingleActionAgent\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AGENT_TO_CLASS\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_chain, load_chain_from_config\n\u001b[1;32m     17\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__file__\u001b[39m)\n\u001b[1;32m     19\u001b[0m URL_BASE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/hwchase17/langchain-hub/master/agents/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/chains/loading.py:18\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     _load_output_parser,\n\u001b[1;32m     13\u001b[0m     load_prompt,\n\u001b[1;32m     14\u001b[0m     load_prompt_from_config,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReduceDocumentsChain\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m APIChain\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chain\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombine_documents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmap_reduce\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MapReduceDocumentsChain\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/chains/api/base.py:56\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextRequestsWrapper\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;129m@deprecated\u001b[39m(\n\u001b[1;32m     59\u001b[0m         since\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.2.13\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     60\u001b[0m         message\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     )\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAPIChain\u001b[39;00m(Chain):\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"Chain that makes API calls and summarizes the responses to answer a question.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m        *Security Note*: This API chain uses the requests toolkit\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m                event[\"messages\"][-1].pretty_print()\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_community/utilities/requests.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m asynccontextmanager\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, AsyncGenerator, Dict, Literal, Optional, Union\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maiohttp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, ConfigDict\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/aiohttp/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Tuple\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hdrs \u001b[38;5;28;01mas\u001b[39;00m hdrs\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     BaseConnector \u001b[38;5;28;01mas\u001b[39;00m BaseConnector,\n\u001b[1;32m      8\u001b[0m     ClientConnectionError \u001b[38;5;28;01mas\u001b[39;00m ClientConnectionError,\n\u001b[1;32m      9\u001b[0m     ClientConnectorCertificateError \u001b[38;5;28;01mas\u001b[39;00m ClientConnectorCertificateError,\n\u001b[1;32m     10\u001b[0m     ClientConnectorError \u001b[38;5;28;01mas\u001b[39;00m ClientConnectorError,\n\u001b[1;32m     11\u001b[0m     ClientConnectorSSLError \u001b[38;5;28;01mas\u001b[39;00m ClientConnectorSSLError,\n\u001b[1;32m     12\u001b[0m     ClientError \u001b[38;5;28;01mas\u001b[39;00m ClientError,\n\u001b[1;32m     13\u001b[0m     ClientHttpProxyError \u001b[38;5;28;01mas\u001b[39;00m ClientHttpProxyError,\n\u001b[1;32m     14\u001b[0m     ClientOSError \u001b[38;5;28;01mas\u001b[39;00m ClientOSError,\n\u001b[1;32m     15\u001b[0m     ClientPayloadError \u001b[38;5;28;01mas\u001b[39;00m ClientPayloadError,\n\u001b[1;32m     16\u001b[0m     ClientProxyConnectionError \u001b[38;5;28;01mas\u001b[39;00m ClientProxyConnectionError,\n\u001b[1;32m     17\u001b[0m     ClientRequest \u001b[38;5;28;01mas\u001b[39;00m ClientRequest,\n\u001b[1;32m     18\u001b[0m     ClientResponse \u001b[38;5;28;01mas\u001b[39;00m ClientResponse,\n\u001b[1;32m     19\u001b[0m     ClientResponseError \u001b[38;5;28;01mas\u001b[39;00m ClientResponseError,\n\u001b[1;32m     20\u001b[0m     ClientSession \u001b[38;5;28;01mas\u001b[39;00m ClientSession,\n\u001b[1;32m     21\u001b[0m     ClientSSLError \u001b[38;5;28;01mas\u001b[39;00m ClientSSLError,\n\u001b[1;32m     22\u001b[0m     ClientTimeout \u001b[38;5;28;01mas\u001b[39;00m ClientTimeout,\n\u001b[1;32m     23\u001b[0m     ClientWebSocketResponse \u001b[38;5;28;01mas\u001b[39;00m ClientWebSocketResponse,\n\u001b[1;32m     24\u001b[0m     ContentTypeError \u001b[38;5;28;01mas\u001b[39;00m ContentTypeError,\n\u001b[1;32m     25\u001b[0m     Fingerprint \u001b[38;5;28;01mas\u001b[39;00m Fingerprint,\n\u001b[1;32m     26\u001b[0m     InvalidURL \u001b[38;5;28;01mas\u001b[39;00m InvalidURL,\n\u001b[1;32m     27\u001b[0m     NamedPipeConnector \u001b[38;5;28;01mas\u001b[39;00m NamedPipeConnector,\n\u001b[1;32m     28\u001b[0m     RequestInfo \u001b[38;5;28;01mas\u001b[39;00m RequestInfo,\n\u001b[1;32m     29\u001b[0m     ServerConnectionError \u001b[38;5;28;01mas\u001b[39;00m ServerConnectionError,\n\u001b[1;32m     30\u001b[0m     ServerDisconnectedError \u001b[38;5;28;01mas\u001b[39;00m ServerDisconnectedError,\n\u001b[1;32m     31\u001b[0m     ServerFingerprintMismatch \u001b[38;5;28;01mas\u001b[39;00m ServerFingerprintMismatch,\n\u001b[1;32m     32\u001b[0m     ServerTimeoutError \u001b[38;5;28;01mas\u001b[39;00m ServerTimeoutError,\n\u001b[1;32m     33\u001b[0m     TCPConnector \u001b[38;5;28;01mas\u001b[39;00m TCPConnector,\n\u001b[1;32m     34\u001b[0m     TooManyRedirects \u001b[38;5;28;01mas\u001b[39;00m TooManyRedirects,\n\u001b[1;32m     35\u001b[0m     UnixConnector \u001b[38;5;28;01mas\u001b[39;00m UnixConnector,\n\u001b[1;32m     36\u001b[0m     WSServerHandshakeError \u001b[38;5;28;01mas\u001b[39;00m WSServerHandshakeError,\n\u001b[1;32m     37\u001b[0m     request \u001b[38;5;28;01mas\u001b[39;00m request,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcookiejar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CookieJar \u001b[38;5;28;01mas\u001b[39;00m CookieJar, DummyCookieJar \u001b[38;5;28;01mas\u001b[39;00m DummyCookieJar\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FormData \u001b[38;5;28;01mas\u001b[39;00m FormData\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/aiohttp/client.py:91\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttp_websocket\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WSHandshakeError, WSMessage, ws_ext_gen, ws_ext_parse\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreams\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FlowControlDataQueue\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtracing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trace, TraceConfig\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypedefs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JSONEncoder, LooseCookies, LooseHeaders, StrOrURL\n\u001b[1;32m     94\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# client_exceptions\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClientConnectionError\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    131\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/aiohttp/tracing.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Awaitable, Optional, Protocol, Type, TypeVar\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mattr\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maiosignal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Signal\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultidict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CIMultiDict\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myarl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m URL\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/aiosignal/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfrozenlist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FrozenList\n\u001b[1;32m      3\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.3.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSignal\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/frozenlist/__init__.py:90\u001b[0m\n\u001b[1;32m     86\u001b[0m PyFrozenList \u001b[38;5;241m=\u001b[39m FrozenList\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_frozenlist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FrozenList \u001b[38;5;28;01mas\u001b[39;00m CFrozenList  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m NO_EXTENSIONS:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     93\u001b[0m         FrozenList \u001b[38;5;241m=\u001b[39m CFrozenList  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1354\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1316\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1256\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1524\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1498\u001b[0m, in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1601\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1644\u001b[0m, in \u001b[0;36m_fill_cache\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Part 1: Imports and Basic Setup\n",
    "# Import required libraries for data processing, database operations, language models, and environment variables\n",
    "import os\n",
    "from typing import Dict, List, Optional, TypedDict, Literal, Union, Annotated\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import json\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AnyMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\n",
    "import sqlite3\n",
    "import re\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Load API keys from environment file\n",
    "load_dotenv('api_key.env')\n",
    "\n",
    "# Initialize memory for state management\n",
    "memory = {}  # Using a simple dictionary for in-memory storage\n",
    "\n",
    "# Part 2: Type Definitions and Base Classes\n",
    "class QueryType(Enum):\n",
    "    DIRECT_SQL = \"direct_sql\" \n",
    "    ANALYSIS = \"analysis\"\n",
    "\n",
    "@dataclass\n",
    "class QueryClassification:\n",
    "    type: QueryType\n",
    "    explanation: str\n",
    "    raw_response: str\n",
    "    confidence: float = 1.0\n",
    "\n",
    "class AnalysisState(TypedDict):\n",
    "    user_query: str\n",
    "    query_classification: Dict\n",
    "    decomposed_questions: List[str]\n",
    "    sql_results: Dict\n",
    "    analysis: str\n",
    "    final_output: Dict\n",
    "    token_usage: Dict\n",
    "    processing_time: float\n",
    "    agent_states: Dict\n",
    "    raw_responses: Dict\n",
    "    messages: List[AnyMessage]\n",
    "\n",
    "class ConfigError(Exception):\n",
    "    \"\"\"Custom exception for configuration errors\"\"\"\n",
    "    pass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    db_path: str = \"final_working_database.db\"\n",
    "    sqlite_path: str = \"sqlite:///final_working_database.db\"\n",
    "    model_name: str = \"claude-3-sonnet-20240229\"\n",
    "    confidence_threshold: float = 0.85  # High confidence threshold for autonomous decisions\n",
    "    \n",
    "    @property\n",
    "    def api_key(self) -> str:\n",
    "        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ConfigError(\"ANTHROPIC_API_KEY not found in api_key.env file\")\n",
    "        return api_key\n",
    "\n",
    "# Part 3: Prompt Templates\n",
    "QUERY_CLASSIFIER_PROMPT = \"\"\"You are a query classifier that determines if a stock market question:\n",
    "1. Can be answered with a direct SQL query\n",
    "2. Needs complex analysis\n",
    "\n",
    "Respond in JSON format:\n",
    "{\n",
    "    \"type\": \"direct_sql\" | \"analysis\",\n",
    "    \"explanation\": \"brief explanation of classification\",\n",
    "    \"confidence\": <float between 0-1>,\n",
    "    \"needs_clarification\": {\n",
    "        \"required\": <boolean>,\n",
    "        \"details\": \"description of ambiguity or missing information\",\n",
    "        \"suggested_questions\": [\"list of clarifying questions\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "Examples:\n",
    "1. \"What is the average stock price of Company X over the last year?\" \n",
    "   - This can be answered with a direct SQL query.\n",
    "2. \"What factors influenced the stock price fluctuations of Company Y?\" \n",
    "   - This requires complex analysis.\n",
    "3. \"How many shares of Company Z were traded last quarter?\" \n",
    "   - This can be answered with a direct SQL query.\n",
    "4. \"What is the correlation between the stock prices of Company A and Company B?\" \n",
    "   - This requires complex analysis.\n",
    "\"\"\"\n",
    "\n",
    "SQL_AGENT_PROMPT = \"\"\"You are an expert financial database analyst. Your task is to:\n",
    "1. Analyze stock market queries\n",
    "2. Create appropriate SQL queries using the provided database schema\n",
    "3. Provide clear results\n",
    "\n",
    "Here is the database schema:\n",
    "{schema}\n",
    "\n",
    "If you encounter any ambiguity or data limitations:\n",
    "1. Clearly explain the issue\n",
    "2. Specify what clarification is needed\n",
    "3. Suggest possible alternatives\n",
    "\n",
    "Your responses should include:\n",
    "1. Confidence level (0-1)\n",
    "2. Any clarification needed\n",
    "3. Thought process\n",
    "4. SQL query (if possible)\n",
    "5. Result interpretation\"\"\"\n",
    "\n",
    "ANALYST_PROMPT = \"\"\"You are an expert financial analyst. Analyze the provided SQL results and provide insights.\n",
    "\n",
    "If you encounter:\n",
    "- Unclear patterns\n",
    "- Multiple possible interpretations\n",
    "- Need for additional context\n",
    "- Insufficient data\n",
    "\n",
    "Clearly state:\n",
    "1. What additional information would help\n",
    "2. Why it's needed\n",
    "3. How it would improve the analysis\n",
    "\n",
    "Focus on:\n",
    "1. Price trends and patterns\n",
    "2. Volume analysis\n",
    "3. Technical indicators\n",
    "4. Risk assessment\n",
    "5. Notable patterns\n",
    "\n",
    "Be specific and data-driven in your analysis.\"\"\"\n",
    "\n",
    "# Part 4: Main StockAnalyzer Class\n",
    "class StockAnalyzer:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.conn = sqlite3.connect(config.db_path)\n",
    "        self.schema = self._get_database_schema()  # Get schema first\n",
    "        self.db = self._init_database()\n",
    "        self.llm = self._init_llm()\n",
    "        \n",
    "        # Initialize conversation memories for different components\n",
    "        self.classifier_memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        self.sql_memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        self.analyst_memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        self.sql_agent = self._setup_sql_agent()\n",
    "        self.token_usage = {\"prompt_tokens\": 0, \"completion_tokens\": 0}\n",
    "        self.anthropic_client = Anthropic(api_key=config.api_key)\n",
    "        self.agent_states = {}\n",
    "        self.raw_responses = {}\n",
    "        \n",
    "        # Cache for storing previous query results\n",
    "        self.query_cache = {}\n",
    "\n",
    "    def _init_database(self) -> SQLDatabase:\n",
    "        try:\n",
    "            return SQLDatabase.from_uri(self.config.sqlite_path)\n",
    "        except Exception as e:\n",
    "            raise ConfigError(f\"Database initialization failed: {str(e)}\")\n",
    "\n",
    "    def _get_database_schema(self) -> str:\n",
    "        try:\n",
    "            cursor = self.conn.cursor()\n",
    "            tables = cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
    "            \n",
    "            schema = []\n",
    "            for table in tables:\n",
    "                table_name = table[0]\n",
    "                columns = cursor.execute(f\"PRAGMA table_info({table_name});\").fetchall()\n",
    "                schema.append(f\"Table: {table_name}\")\n",
    "                schema.append(\"Columns:\")\n",
    "                for col in columns:\n",
    "                    schema.append(f\"  - {col[1]} ({col[2]})\")\n",
    "                schema.append(\"\")\n",
    "                \n",
    "            return \"\\n\".join(schema)\n",
    "        except Exception as e:\n",
    "            raise ConfigError(f\"Failed to get database schema: {str(e)}\")\n",
    "\n",
    "    def _init_llm(self) -> ChatAnthropic:\n",
    "        return ChatAnthropic(\n",
    "            model=self.config.model_name,\n",
    "            temperature=0,\n",
    "            api_key=self.config.api_key\n",
    "        )\n",
    "\n",
    "    def _setup_sql_agent(self):\n",
    "        toolkit = SQLDatabaseToolkit(db=self.db, llm=self.llm)\n",
    "        return create_sql_agent(\n",
    "            llm=self.llm,\n",
    "            toolkit=toolkit,\n",
    "            agent_type=\"zero-shot-react-description\",\n",
    "            verbose=True,\n",
    "            memory=self.sql_memory,\n",
    "            prefix=SQL_AGENT_PROMPT.format(schema=self.schema)\n",
    "        )\n",
    "\n",
    "    def _get_user_clarification(self, prompt: str) -> str:\n",
    "        return input(f\"\\n{prompt}\\nPlease provide clarification: \")\n",
    "\n",
    "    def analyze(self, query: str) -> Dict:\n",
    "        # Check if we have cached results for this query\n",
    "        if query in self.query_cache:\n",
    "            print(\"Using cached results...\")\n",
    "            return self.query_cache[query]\n",
    "            \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            self.token_usage = {\"prompt_tokens\": 0, \"completion_tokens\": 0}\n",
    "            self.agent_states = {}\n",
    "            self.raw_responses = {}\n",
    "            \n",
    "            classification = self._classify_query(query)\n",
    "            \n",
    "            if classification.type == QueryType.DIRECT_SQL and classification.confidence >= self.config.confidence_threshold:\n",
    "                result = self._direct_sql_query(query)\n",
    "                self.query_cache[query] = result\n",
    "                return result\n",
    "            \n",
    "            decomposed_questions = self._decompose_question(query)\n",
    "            sql_results = self._run_sql_analysis(decomposed_questions)\n",
    "            \n",
    "            for result in sql_results.values():\n",
    "                if isinstance(result.get('result'), str) and 'error' in result.get('result', '').lower():\n",
    "                    clarification = self._get_user_clarification(\n",
    "                        f\"Error in SQL execution: {result['result']}\\nHow would you like to proceed?\"\n",
    "                    )\n",
    "                    if clarification:\n",
    "                        result = self._retry_sql_query(result['question'], clarification)\n",
    "                        if result:\n",
    "                            sql_results[result['question']] = result\n",
    "            \n",
    "            analysis = self._analyze_results(query, sql_results)\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            final_output = {\n",
    "                \"query_type\": classification.type.value,\n",
    "                \"user_query\": query,\n",
    "                \"query_classification\": {\n",
    "                    \"type\": classification.type.value,\n",
    "                    \"explanation\": classification.explanation,\n",
    "                    \"confidence\": classification.confidence,\n",
    "                    \"raw_response\": classification.raw_response\n",
    "                },\n",
    "                \"sub_questions\": decomposed_questions,\n",
    "                \"sql_analysis\": sql_results,\n",
    "                \"expert_analysis\": analysis,\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "                \"token_usage\": self.token_usage,\n",
    "                \"processing_time\": processing_time,\n",
    "                \"agent_states\": self.agent_states,\n",
    "                \"raw_responses\": self.raw_responses\n",
    "            }\n",
    "            \n",
    "            filename = f\"{query[:50].replace(' ', '_').lower()}_analysis.json\"\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(final_output, f, indent=2)\n",
    "                \n",
    "            # Cache the results\n",
    "            self.query_cache[query] = final_output\n",
    "            return final_output\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"query\": query}\n",
    "        finally:\n",
    "            self.conn.close()\n",
    "\n",
    "    def _classify_query(self, query: str) -> QueryClassification:\n",
    "        # Check memory for similar queries\n",
    "        chat_history = self.classifier_memory.load_memory_variables({})[\"chat_history\"]\n",
    "        for message in chat_history:\n",
    "            if isinstance(message, HumanMessage) and query.lower() in message.content.lower():\n",
    "                # Found similar query, use cached classification\n",
    "                print(\"Using cached classification...\")\n",
    "                return self.query_cache.get(message.content, {}).get(\"query_classification\")\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([\n",
    "                SystemMessage(content=QUERY_CLASSIFIER_PROMPT),\n",
    "                HumanMessage(content=f\"Classify this question: {query}\")\n",
    "            ])\n",
    "            \n",
    "            # Save interaction to memory\n",
    "            self.classifier_memory.save_context(\n",
    "                {\"input\": query},\n",
    "                {\"output\": response.content}\n",
    "            )\n",
    "            \n",
    "            self._update_token_usage(response)\n",
    "            classification = json.loads(response.content)\n",
    "            \n",
    "            self.raw_responses['classification'] = response.content\n",
    "            \n",
    "            if classification.get('needs_clarification', {}).get('required', False):\n",
    "                details = classification['needs_clarification']['details']\n",
    "                questions = classification['needs_clarification']['suggested_questions']\n",
    "                clarification = self._get_user_clarification(\n",
    "                    f\"Need clarification: {details}\\n\\nSuggested questions:\\n\" + \n",
    "                    \"\\n\".join(f\"- {q}\" for q in questions)\n",
    "                )\n",
    "                return self._classify_query(f\"{query} {clarification}\")\n",
    "            \n",
    "            return QueryClassification(\n",
    "                type=QueryType(classification['type']),\n",
    "                explanation=classification['explanation'],\n",
    "                confidence=classification.get('confidence', 1.0),\n",
    "                raw_response=response.content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return QueryClassification(\n",
    "                type=QueryType.ANALYSIS,\n",
    "                explanation=f\"Classification failed: {str(e)}\",\n",
    "                confidence=0.0,\n",
    "                raw_response=str(e)\n",
    "            )\n",
    "\n",
    "    def _direct_sql_query(self, query: str) -> Dict:\n",
    "        # Check memory for similar queries\n",
    "        chat_history = self.sql_memory.load_memory_variables({})[\"chat_history\"]\n",
    "        for message in chat_history:\n",
    "            if isinstance(message, HumanMessage) and query.lower() in message.content.lower():\n",
    "                print(\"Using cached SQL query results...\")\n",
    "                return self.query_cache.get(message.content, {})\n",
    "        \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            result = self.sql_agent.invoke({\"input\": query})\n",
    "            self._update_token_usage(result)\n",
    "            \n",
    "            self.agent_states['direct_sql'] = result\n",
    "            \n",
    "            thought = self._extract_thought(result['output'])\n",
    "            sql = self._extract_sql(result['output'])\n",
    "            \n",
    "            if not sql:\n",
    "                clarification = self._get_user_clarification(\n",
    "                    \"Could not generate SQL query. Please provide guidance on what data you're looking for:\"\n",
    "                )\n",
    "                result = self.sql_agent.invoke({\"input\": f\"{query} {clarification}\"})\n",
    "                sql = self._extract_sql(result['output'])\n",
    "            \n",
    "            try:\n",
    "                sql = sql.split(';')[0] + ';'\n",
    "                df = pd.read_sql_query(sql, self.conn)\n",
    "                formatted_results = df.to_dict('records')\n",
    "            except Exception as e:\n",
    "                clarification = self._get_user_clarification(\n",
    "                    f\"Error executing SQL: {str(e)}\\nHow would you like to modify the query?\"\n",
    "                )\n",
    "                try:\n",
    "                    df = pd.read_sql_query(clarification, self.conn)\n",
    "                    formatted_results = df.to_dict('records')\n",
    "                except Exception as e2:\n",
    "                    formatted_results = f\"Error executing SQL even after clarification: {str(e2)}\"\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            final_result = {\n",
    "                \"query_type\": \"direct_sql\",\n",
    "                \"user_query\": query,\n",
    "                \"thought_process\": thought,\n",
    "                \"sql_query\": sql,\n",
    "                \"results\": formatted_results,\n",
    "                \"raw_agent_output\": result['output'],\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "                \"token_usage\": self.token_usage,\n",
    "                \"processing_time\": processing_time,\n",
    "                \"agent_state\": result\n",
    "            }\n",
    "            \n",
    "            # Save to memory\n",
    "            self.sql_memory.save_context(\n",
    "                {\"input\": query},\n",
    "                {\"output\": json.dumps(final_result)}\n",
    "            )\n",
    "            \n",
    "            return final_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"query\": query}\n",
    "\n",
    "    def _decompose_question(self, query: str) -> List[str]:\n",
    "        response = self.llm.invoke([\n",
    "            SystemMessage(content=\"Break down this stock analysis question into specific sub-questions that can be answered with SQL queries:\"),\n",
    "            HumanMessage(content=query)\n",
    "        ])\n",
    "        \n",
    "        self._update_token_usage(response)\n",
    "        self.raw_responses['decomposition'] = response.content\n",
    "        \n",
    "        questions = [\n",
    "            q.strip().split(\". \", 1)[1] if \". \" in q else q.strip()\n",
    "            for q in response.content.split(\"\\n\")\n",
    "            if q.strip() and q[0].isdigit()\n",
    "        ]\n",
    "        \n",
    "        if not questions:\n",
    "            clarification = self._get_user_clarification(\n",
    "                \"Could not break down the question. Please specify what aspects you want to analyze:\"\n",
    "            )\n",
    "            return self._decompose_question(f\"{query} {clarification}\")\n",
    "        \n",
    "        return questions\n",
    "\n",
    "    def _run_sql_analysis(self, questions: List[str]) -> Dict:\n",
    "        results = {}\n",
    "        agent_states = {}\n",
    "        \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            # Check memory for similar questions\n",
    "            chat_history = self.sql_memory.load_memory_variables({})[\"chat_history\"]\n",
    "            cached_result = None\n",
    "            for message in chat_history:\n",
    "                if isinstance(message, HumanMessage) and question.lower() in message.content.lower():\n",
    "                    print(f\"Using cached results for sub-question {i}...\")\n",
    "                    cached_result = self.query_cache.get(message.content)\n",
    "                    break\n",
    "            \n",
    "            if cached_result:\n",
    "                results[f\"question_{i}\"] = cached_result\n",
    "            try:\n",
    "                result = self.sql_agent.invoke({\"input\": question})\n",
    "                self._update_token_usage(result)\n",
    "                \n",
    "                agent_states[f\"question_{i}\"] = result\n",
    "                \n",
    "                thought = self._extract_thought(result['output'])\n",
    "                sql = self._extract_sql(result['output'])\n",
    "                \n",
    "                if not sql:\n",
    "                    clarification = self._get_user_clarification(\n",
    "                        f\"Could not generate SQL for: {question}\\nPlease provide guidance:\"\n",
    "                    )\n",
    "                    result = self.sql_agent.invoke({\"input\": f\"{question} {clarification}\"})\n",
    "                    sql = self._extract_sql(result['output'])\n",
    "                \n",
    "                try:\n",
    "                    sql = sql.split(';')[0] + ';'\n",
    "                    df = pd.read_sql_query(sql, self.conn)\n",
    "                    parsed_result = df.to_dict('records')\n",
    "                except Exception as e:\n",
    "                    clarification = self._get_user_clarification(\n",
    "                        f\"Error executing SQL for: {question}\\n{str(e)}\\nHow would you like to modify the query?\"\n",
    "                    )\n",
    "                    try:\n",
    "                        df = pd.read_sql_query(clarification, self.conn)\n",
    "                        parsed_result = df.to_dict('records')\n",
    "                        sql = clarification\n",
    "                    except Exception as e2:\n",
    "                        parsed_result = f\"Error executing SQL even after clarification: {str(e2)}\"\n",
    "                \n",
    "                results[f\"question_{i}\"] = {\n",
    "                    \"question\": question,\n",
    "                    \"thought\": thought if thought else \"No thought process provided\",\n",
    "                    \"sql\": sql if sql else \"No SQL query provided\",\n",
    "                    \"result\": parsed_result,\n",
    "                    \"raw_output\": result['output']\n",
    "                }\n",
    "                    \n",
    "            except Exception as e:\n",
    "                results[f\"question_{i}\"] = {\n",
    "                    \"error\": str(e),\n",
    "                    \"question\": question\n",
    "                }\n",
    "        \n",
    "        self.agent_states['sql_analysis'] = agent_states\n",
    "        return results\n",
    "\n",
    "    def _analyze_results(self, query: str, sql_results: Dict) -> str:\n",
    "        results_context = json.dumps(sql_results, indent=2)\n",
    "        response = self.llm.invoke([\n",
    "            SystemMessage(content=ANALYST_PROMPT),\n",
    "            HumanMessage(content=f\"\"\"\n",
    "            Original Question: {query}\n",
    "            \n",
    "            Analysis Results:\n",
    "            {results_context}\n",
    "            \n",
    "            Provide a comprehensive analysis.\"\"\")\n",
    "        ])\n",
    "        \n",
    "        self._update_token_usage(response)\n",
    "        self.raw_responses['analysis'] = response.content\n",
    "        \n",
    "        if len(response.content.strip()) < 100:  # If analysis is too short\n",
    "            clarification = self._get_user_clarification(\n",
    "                \"Analysis seems incomplete. What specific aspects would you like to focus on?\"\n",
    "            )\n",
    "            return self._analyze_results(f\"{query} {clarification}\", sql_results)\n",
    "        \n",
    "        return response.content\n",
    "\n",
    "    def _retry_sql_query(self, question: str, clarification: str) -> Dict:\n",
    "        try:\n",
    "            result = self.sql_agent.invoke({\"input\": f\"{question} {clarification}\"})\n",
    "            sql = self._extract_sql(result['output'])\n",
    "            \n",
    "            if sql:\n",
    "                df = pd.read_sql_query(sql, self.conn)\n",
    "                return {\n",
    "                    \"question\": question,\n",
    "                    \"thought\": self._extract_thought(result['output']),\n",
    "                    \"sql\": sql,\n",
    "                    \"result\": df.to_dict('records'),\n",
    "                    \"raw_output\": result['output']\n",
    "                }\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def _update_token_usage(self, response):\n",
    "        if hasattr(response, '_raw_response') and 'usage' in response._raw_response:\n",
    "            usage = response._raw_response['usage']\n",
    "            self.token_usage[\"prompt_tokens\"] += usage.get('input_tokens', 0)\n",
    "            self.token_usage[\"completion_tokens\"] += usage.get('output_tokens', 0)\n",
    "        elif isinstance(response, dict) and 'usage' in response:\n",
    "            usage = response['usage']\n",
    "            self.token_usage[\"prompt_tokens\"] += usage.get('input_tokens', 0)\n",
    "            self.token_usage[\"completion_tokens\"] += usage.get('output_tokens', 0)\n",
    "        elif hasattr(response, 'usage'):\n",
    "            usage = response.usage\n",
    "            self.token_usage[\"prompt_tokens\"] += usage.input_tokens if hasattr(usage, 'input_tokens') else 0\n",
    "            self.token_usage[\"completion_tokens\"] += usage.output_tokens if hasattr(usage, 'output_tokens') else 0\n",
    "        else:\n",
    "            message = response.content if hasattr(response, 'content') else str(response)\n",
    "            result = self.anthropic_client.messages.create(\n",
    "                model=self.config.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": message}],\n",
    "                max_tokens=1\n",
    "            )\n",
    "            if hasattr(result, 'usage'):\n",
    "                self.token_usage[\"prompt_tokens\"] += result.usage.input_tokens\n",
    "                self.token_usage[\"completion_tokens\"] += result.usage.output_tokens\n",
    "\n",
    "    def _extract_thought(self, text: str) -> str:\n",
    "        if \"Thought:\" in text:\n",
    "            return text.split(\"Thought:\")[1].split(\"SQL\")[0].strip()\n",
    "        return \"\"\n",
    "\n",
    "    def _extract_sql(self, text: str) -> str:\n",
    "        if \"SQL:\" in text:\n",
    "            sql_part = text.split(\"SQL:\")[1]\n",
    "            if \"SQLResult:\" in sql_part:\n",
    "                return sql_part.split(\"SQLResult:\")[0].strip()\n",
    "            if \"Final Answer:\" in sql_part:\n",
    "                return sql_part.split(\"Final Answer:\")[0].strip()\n",
    "            return sql_part.strip()\n",
    "        return \"\"\n",
    "\n",
    "def format_output(results: Dict) -> str:\n",
    "    output = []\n",
    "    output.append(\"=== Stock Analysis Results ===\")\n",
    "    output.append(f\"\\nQuery: {results.get('user_query', 'N/A')}\")\n",
    "    \n",
    "    output.append(f\"\\nProcessing Time: {results.get('processing_time', 0):.2f} seconds\")\n",
    "    token_usage = results.get('token_usage', {})\n",
    "    output.append(f\"Token Usage:\")\n",
    "    output.append(f\"  Prompt Tokens: {token_usage.get('prompt_tokens', 0)}\")\n",
    "    output.append(f\"  Completion Tokens: {token_usage.get('completion_tokens', 0)}\")\n",
    "    output.append(f\"  Total Tokens: {token_usage.get('prompt_tokens', 0) + token_usage.get('completion_tokens', 0)}\")\n",
    "    \n",
    "    if \"error\" in results:\n",
    "        output.append(f\"\\nError: {results['error']}\")\n",
    "        return \"\\n\".join(output)\n",
    "    \n",
    "    if results.get('query_type') == 'direct_sql':\n",
    "        output.append(f\"\\nThought Process: {results.get('thought_process', 'N/A')}\")\n",
    "        output.append(f\"\\nSQL Query: {results.get('sql_query', 'N/A')}\")\n",
    "        output.append(\"\\nResults:\")\n",
    "        if isinstance(results.get('results'), list):\n",
    "            df = pd.DataFrame(results['results'])\n",
    "            output.append(str(df))\n",
    "        else:\n",
    "            output.append(str(results.get('results', 'No results available')))\n",
    "    else:\n",
    "        output.append(\"\\nSub-Questions:\")\n",
    "        for i, q in enumerate(results.get('sub_questions', []), 1):\n",
    "            output.append(f\"{i}. {q}\")\n",
    "        \n",
    "        output.append(\"\\nSQL Analysis:\")\n",
    "        for key, data in results.get('sql_analysis', {}).items():\n",
    "            output.append(f\"\\nQuestion: {data.get('question', 'N/A')}\")\n",
    "            if 'error' not in data:\n",
    "                output.append(f\"Thought Process: {data.get('thought', 'N/A')}\")\n",
    "                output.append(f\"SQL Query: {data.get('sql', 'N/A')}\")\n",
    "                try:\n",
    "                    if isinstance(data.get('result'), (list, dict)):\n",
    "                        df = pd.DataFrame(data['result'])\n",
    "                        output.append(str(df))\n",
    "                    else:\n",
    "                        output.append(f\"Results: {data.get('result', 'No results available')}\")\n",
    "                except:\n",
    "                    output.append(f\"Results: {data.get('result', 'No results available')}\")\n",
    "            else:\n",
    "                output.append(f\"Error: {data['error']}\")\n",
    "        \n",
    "        output.append(\"\\nExpert Analysis:\")\n",
    "        output.append(results.get('expert_analysis', 'No analysis available'))\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "def analyze_stock_query(query: str) -> str:\n",
    "    try:\n",
    "        config = Config()  # Removed the human_in_the_loop argument\n",
    "        analyzer = StockAnalyzer(config)\n",
    "        results = analyzer.analyze(query)\n",
    "        \n",
    "        if results and \"error\" not in results:\n",
    "            formatted_output = format_output(results)\n",
    "            filename = f\"{query[:50].replace(' ', '_').lower()}_analysis.json\"\n",
    "            return formatted_output + f\"\\n\\nDetailed results saved to {filename}\"\n",
    "        else:\n",
    "            return f\"Error: {results.get('error', 'Unknown error occurred')}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error during analysis: {str(e)}\"\n",
    "\n",
    "# Part 6: Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    test_queries = [\n",
    "        \"How does the EBITDA margin trend compare between AC Wailea and Surfrider Malibu properties over the past 12 months, and what are the key drivers behind any significant variances from budgeted figures?\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nProcessing: {query}\")\n",
    "        print(\"=\" * 50) \n",
    "        result = analyze_stock_query(query)  # Removed the human_in_the_loop argument\n",
    "        print(result)\n",
    "        print(\"\\n\" + \"=\"*50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Displaying detailed analysis for: How does the EBITDA margin trend compare between AC Wailea and Surfrider Malibu properties over the past 12 months, and what are the key drivers behind any significant variances from budgeted figures?\n",
      "\n",
      "=== DETAILED ANALYSIS REPORT ===\n",
      "\n",
      "\n",
      "Query Type:analysis\n",
      "\n",
      "User Query:How does the EBITDA margin trend compare between AC Wailea and Surfrider Malibu properties over the past 12 months, and what are the key drivers behind any significant variances from budgeted figures?\n",
      "\n",
      "Query Classification:Type:\n",
      "analysis\n",
      "Explanation:\n",
      "This question requires analyzing EBITDA margin trends and identifying key drivers behind variances from budgeted figures for two specific properties over the past 12 months. It cannot be directly answered with a simple SQL query.\n",
      "Confidence:\n",
      "0.9\n",
      "Raw Response:\n",
      "{\n",
      "    \"type\": \"analysis\",\n",
      "    \"explanation\": \"This question requires analyzing EBITDA margin trends and identifying key drivers behind variances from budgeted figures for two specific properties over the past 12 months. It cannot be directly answered with a simple SQL query.\",\n",
      "    \"confidence\": 0.9,\n",
      "    \"needs_clarification\": {\n",
      "        \"required\": false,\n",
      "        \"details\": \"\",\n",
      "        \"suggested_questions\": []\n",
      "    }\n",
      "}\n",
      "\n",
      "Sub Questions: Calculate the monthly EBITDA margin for AC Wailea and Surfrider Malibu properties over the past 12 months:\n",
      " Compare the actual EBITDA margins with the budgeted figures:\n",
      " Analyze the key drivers behind significant variances (if any) by looking at revenue and expense line items:\n",
      "\n",
      "Sql Analysis:Question 1:\n",
      "    Error:\n",
      "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `The query successfully calculated the monthly EBITDA and EBITDA margin for the AC Wailea and Surfrider Malibu properties over the available time period in the database.\n",
      "\n",
      "Some key observations from the results:\n",
      "\n",
      "- For AC Wailea, the EBITDA margin ranges from around -270% to +18% over the months shown. Many months have negative EBITDA due to expenses exceeding revenues.\n",
      "\n",
      "- For Surfrider Malibu, the EBITDA margin ranges from around -105% to -33%. All months shown have negative EBITDA.\n",
      "\n",
      "- The most recent months of data are October 2024 for both properties.\n",
      "\n",
      "- There are some very large negative EBITDA values, like -$3.6M for AC Wailea in June 2024, indicating very high expenses compared to revenues in certain months.\n",
      "\n",
      "- The results cover over 2 years of monthly data from mid-2021 through October 2024.\n",
      "\n",
      "Overall, this provides the requested monthly EBITDA and margin metrics for analyzing the financial performance of these two properties over an extended time period. The wide swings in EBITDA margin each month likely warrant further investigation into the key revenue and expense drivers.`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE\n",
      "    Question:\n",
      "Calculate the monthly EBITDA margin for AC Wailea and Surfrider Malibu properties over the past 12 months:\n",
      "Question 2:\n",
      "    Error:\n",
      "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `The query result shows the following:\n",
      "\n",
      "SQL_Account_Group_Name: EBITDA\n",
      "actual_ebitda: 640474.49\n",
      "budget_ebitda: 876679.7\n",
      "actual_ebitda_margin_pct: 0.09 (9%)\n",
      "budget_ebitda_margin_pct: 0.13 (13%)  \n",
      "\n",
      "This means for the month of October 2024:\n",
      "\n",
      "1. The actual EBITDA value was 640,474.49\n",
      "2. The budgeted EBITDA value was 876,679.7 \n",
      "3. The actual EBITDA margin percentage was 9% of total revenue\n",
      "4. The budgeted EBITDA margin percentage was 13% of total revenue\n",
      "\n",
      "So the actual EBITDA margin of 9% was lower than the budgeted 13% for that month.\n",
      "\n",
      "In summary, the query successfully compared the actual EBITDA margins against the budgeted figures for October 2024, showing that actuals underperformed the budget that month.`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE\n",
      "    Question:\n",
      "Compare the actual EBITDA margins with the budgeted figures:\n",
      "Question 3:\n",
      "    Error:\n",
      "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'prompt is too long: 219811 tokens > 200000 maximum'}}\n",
      "    Question:\n",
      "Analyze the key drivers behind significant variances (if any) by looking at revenue and expense line items:\n",
      "\n",
      "Expert Analysis:Based on the provided SQL results, here is my analysis:\n",
      "\n",
      "1. Price Trends and Patterns:\n",
      "The data does not provide enough information to analyze price trends or patterns for the properties. Additional data on room rates, occupancy rates, and revenue per available room (RevPAR) would be needed.\n",
      "\n",
      "2. Volume Analysis: \n",
      "Without data on occupancy levels, number of rooms sold, or other volume metrics, it is difficult to assess volume drivers behind the EBITDA figures.\n",
      "\n",
      "3. Technical Indicators:\n",
      "The key technical indicator provided is the EBITDA margin percentage. For AC Wailea, this ranged from around -270% to +18% over the months shown, indicating very volatile profitability. For Surfrider Malibu, all months had negative EBITDA margins ranging from -105% to -33%.\n",
      "\n",
      "4. Risk Assessment:\n",
      "The wide swings in monthly EBITDA margins, including many months of negative EBITDA, suggest both properties faced significant risks in covering their operating costs from revenues during this period. The large negative EBITDA values like -$3.6M for AC Wailea indicate very high expense levels relative to revenues in certain months, which is a major risk factor.\n",
      "\n",
      "5. Notable Patterns:\n",
      "A notable pattern is that Surfrider Malibu had negative EBITDA every month shown, while AC Wailea had a mix of positive and negative months. This could indicate more systemic issues at Surfrider in controlling costs relative to revenues.\n",
      "\n",
      "The actual EBITDA margin of 9% for October 2024 underperformed the 13% budgeted margin, but without visibility into the revenue and expense line items, it's difficult to pinpoint the drivers behind this variance.\n",
      "\n",
      "To improve the analysis, additional data would be needed on:\n",
      "\n",
      "1. Occupancy rates and room nights sold\n",
      "2. Average daily rates (ADRs)\n",
      "3. Breakdown of revenue sources (rooms, F&B, other operated departments) \n",
      "4. Detailed operating expense line items\n",
      "5. Capital expenditure levels\n",
      "6. Seasonality factors impacting the monthly figures\n",
      "\n",
      "With this additional context on volumes, pricing, revenue mix, costs, and seasonality, we could better assess the operational drivers behind the EBITDA margin performance and variances from budget. The current data provides some high-level profitability metrics but lacks the granular details to conduct a comprehensive analysis.\n",
      "\n",
      "Timestamp:2024-12-25T12:47:48.775728\n",
      "\n",
      "Token Usage:Prompt Tokens:\n",
      "1,723\n",
      "Completion Tokens:\n",
      "4\n",
      "\n",
      "Processing Time:184.5469150543213\n",
      "\n",
      "Agent States:Sql Analysis:\n",
      "    Question 1:\n",
      "        Input:\n",
      "Calculate the monthly EBITDA margin for AC Wailea and Surfrider Malibu properties over the past 12 months:\n",
      "        Output:\n",
      "The query returned all 0 values, likely because the FINAL_INCOME_SHEET_NEW_SEQ table does not contain the complete income statement data needed to calculate EBITDA for AC Wailea and Surfrider Malibu properties. To investigate further, we should check the FINAL_INCOME_SHEET_TB_NEW and other tables for more comprehensive data, verify the account code patterns and data formats, review data quality reports, try broader filters, and potentially clarify requirements or find an alternative data source if the issues cannot be resolved.\n",
      "\n",
      "Raw Responses:Classification:\n",
      "{\n",
      "    \"type\": \"analysis\",\n",
      "    \"explanation\": \"This question requires analyzing EBITDA margin trends and identifying key drivers behind variances from budgeted figures for two specific properties over the past 12 months. It cannot be directly answered with a simple SQL query.\",\n",
      "    \"confidence\": 0.9,\n",
      "    \"needs_clarification\": {\n",
      "        \"required\": false,\n",
      "        \"details\": \"\",\n",
      "        \"suggested_questions\": []\n",
      "    }\n",
      "}\n",
      "Decomposition:\n",
      "To answer this question, we can break it down into the following sub-queries:\n",
      "\n",
      "1. Calculate the monthly EBITDA margin for AC Wailea and Surfrider Malibu properties over the past 12 months:\n",
      "\n",
      "```sql\n",
      "WITH cte AS (\n",
      "  SELECT \n",
      "    property_name,\n",
      "    DATE_TRUNC('month', date) AS month,\n",
      "    SUM(revenue) AS total_revenue,\n",
      "    SUM(expenses) AS total_expenses\n",
      "  FROM \n",
      "    operating_metrics\n",
      "  WHERE\n",
      "    property_name IN ('AC Wailea', 'Surfrider Malibu')\n",
      "    AND date >= DATE_TRUNC('month', CURRENT_DATE) - INTERVAL '12 months'\n",
      "  GROUP BY\n",
      "    property_name, DATE_TRUNC('month', date)\n",
      ")\n",
      "SELECT\n",
      "  month,\n",
      "  property_name,\n",
      "  ROUND(100.0 * (total_revenue - total_expenses) / total_revenue, 2) AS ebitda_margin\n",
      "FROM\n",
      "  cte\n",
      "ORDER BY\n",
      "  property_name, month;\n",
      "```\n",
      "\n",
      "2. Compare the actual EBITDA margins with the budgeted figures:\n",
      "\n",
      "```sql\n",
      "WITH actual AS (\n",
      "  -- Subquery from step 1\n",
      "),\n",
      "budget AS (\n",
      "  SELECT\n",
      "    property_name,\n",
      "    month,\n",
      "    budgeted_ebitda_margin\n",
      "  FROM\n",
      "    budget_metrics\n",
      "  WHERE\n",
      "    property_name IN ('AC Wailea', 'Surfrider Malibu')\n",
      "    AND month >= DATE_TRUNC('month', CURRENT_DATE) - INTERVAL '12 months'\n",
      ")\n",
      "SELECT\n",
      "  a.month,\n",
      "  a.property_name,\n",
      "  a.ebitda_margin AS actual_margin,\n",
      "  b.budgeted_ebitda_margin,\n",
      "  a.ebitda_margin - b.budgeted_ebitda_margin AS variance\n",
      "FROM\n",
      "  actual a\n",
      "  JOIN budget b ON a.property_name = b.property_name AND a.month = b.month\n",
      "ORDER BY\n",
      "  a.property_name, a.month;\n",
      "```\n",
      "\n",
      "This will give you the actual vs. budgeted EBITDA margins for each property over the past 12 months, along with the variance.\n",
      "\n",
      "3. Analyze the key drivers behind significant variances (if any) by looking at revenue and expense line items:\n",
      "\n",
      "```sql\n",
      "WITH variance AS (\n",
      "  -- Subquery from step 2\n",
      "  SELECT * \n",
      "  FROM previous_query\n",
      "  WHERE ABS(variance) > 5 -- Filter for significant variances, e.g. > 5%\n",
      ")\n",
      "SELECT\n",
      "  v.month,\n",
      "  v.property_name,\n",
      "  r.revenue_type,\n",
      "  r.actual_revenue,\n",
      "  r.budgeted_revenue,\n",
      "  r.actual_revenue - r.budgeted_revenue AS rev_variance,\n",
      "  e.expense_type,\n",
      "  e.actual_expense,\n",
      "  e.budgeted_expense,\n",
      "  e.actual_expense - e.budgeted_expense AS exp_variance\n",
      "FROM\n",
      "  variance v\n",
      "  LEFT JOIN revenue_metrics r\n",
      "    ON v.property_name = r.property_name\n",
      "    AND v.month = r.month  \n",
      "  LEFT JOIN expense_metrics e\n",
      "    ON v.property_name = e.property_name\n",
      "    AND v.month = e.month\n",
      "ORDER BY\n",
      "  v.property_name, v.month;\n",
      "```\n",
      "\n",
      "This query will show you the revenue and expense line items that contributed significantly to the EBITDA margin variance for the months where there was a large deviation from the budget.\n",
      "\n",
      "By combining the results from these queries, you should be able to analyze the EBITDA margin trends for the two properties, identify periods with significant variances from the budget, and understand the key drivers behind those variances.\n",
      "Analysis:\n",
      "Based on the provided SQL results, here is my analysis:\n",
      "\n",
      "1. Price Trends and Patterns:\n",
      "The data does not provide enough information to analyze price trends or patterns for the properties. Additional data on room rates, occupancy rates, and revenue per available room (RevPAR) would be needed.\n",
      "\n",
      "2. Volume Analysis: \n",
      "Without data on occupancy levels, number of rooms sold, or other volume metrics, it is difficult to assess volume drivers behind the EBITDA figures.\n",
      "\n",
      "3. Technical Indicators:\n",
      "The key technical indicator provided is the EBITDA margin percentage. For AC Wailea, this ranged from around -270% to +18% over the months shown, indicating very volatile profitability. For Surfrider Malibu, all months had negative EBITDA margins ranging from -105% to -33%.\n",
      "\n",
      "4. Risk Assessment:\n",
      "The wide swings in monthly EBITDA margins, including many months of negative EBITDA, suggest both properties faced significant risks in covering their operating costs from revenues during this period. The large negative EBITDA values like -$3.6M for AC Wailea indicate very high expense levels relative to revenues in certain months, which is a major risk factor.\n",
      "\n",
      "5. Notable Patterns:\n",
      "A notable pattern is that Surfrider Malibu had negative EBITDA every month shown, while AC Wailea had a mix of positive and negative months. This could indicate more systemic issues at Surfrider in controlling costs relative to revenues.\n",
      "\n",
      "The actual EBITDA margin of 9% for October 2024 underperformed the 13% budgeted margin, but without visibility into the revenue and expense line items, it's difficult to pinpoint the drivers behind this variance.\n",
      "\n",
      "To improve the analysis, additional data would be needed on:\n",
      "\n",
      "1. Occupancy rates and room nights sold\n",
      "2. Average daily rates (ADRs)\n",
      "3. Breakdown of revenue sources (rooms, F&B, other operated departments) \n",
      "4. Detailed operating expense line items\n",
      "5. Capital expenditure levels\n",
      "6. Seasonality factors impacting the monthly figures\n",
      "\n",
      "With this additional context on volumes, pricing, revenue mix, costs, and seasonality, we could better assess the operational drivers behind the EBITDA margin performance and variances from budget. The current data provides some high-level profitability metrics but lacks the granular details to conduct a comprehensive analysis.\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json  # Import the json module for handling JSON data\n",
    "\n",
    "def generate_filename(query: str) -> str:\n",
    "    \"\"\"Generate a filename from the query for the analysis results.\"\"\"\n",
    "    return f\"{query[:50].replace(' ', '_').lower()}_analysis.json\"  # Create a filename based on the query\n",
    "\n",
    "# %% cell 1 code\n",
    "def read_json_file(filename: str) -> dict:\n",
    "    \"\"\"Read and parse the JSON file, returning the data.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)  # Load and return the JSON data\n",
    "    \n",
    "# %% cell 2 code\n",
    "def format_value(value, indent=0):\n",
    "    \"\"\"Recursively format JSON values with proper indentation.\"\"\"\n",
    "    indent_str = \"    \" * indent  # Create indentation string based on the level of nesting\n",
    "    \n",
    "    if isinstance(value, dict):\n",
    "        for k, v in value.items():\n",
    "            key_str = k.replace('_', ' ').title()  # Format the key for display\n",
    "            print(f\"{indent_str}{key_str}:\")\n",
    "            format_value(v, indent + 1)  # Recursively format the value\n",
    "\n",
    "    elif isinstance(value, list):\n",
    "        for item in value:\n",
    "            print(f\"{indent_str}\", end=' ')  # Bullet point for list items\n",
    "            format_value(item, indent + 1)  # Recursively format each item\n",
    "\n",
    "    elif isinstance(value, (int, float)):\n",
    "        print(f\"{value:,}\")  # Print numbers with commas\n",
    "\n",
    "    elif isinstance(value, bool):\n",
    "        print(str(value))  # Print boolean values\n",
    "\n",
    "    elif value is None:\n",
    "        print(\"None\")  # Print 'None' for NoneType\n",
    "\n",
    "    else:\n",
    "        print(str(value).strip())  # Print string values\n",
    "\n",
    "# %% cell 3 code\n",
    "def display_json_details(query: str) -> None:\n",
    "    \"\"\"Display detailed JSON analysis results in a readable format.\"\"\"\n",
    "    try:\n",
    "        filename = generate_filename(query)  # Generate the filename from the query\n",
    "        data = read_json_file(filename)  # Read and parse the JSON file\n",
    "\n",
    "        print(\"\\n=== DETAILED ANALYSIS REPORT ===\\n\")\n",
    "\n",
    "        # Process each top-level key in the JSON data\n",
    "        for key, value in data.items():\n",
    "            print(f\"\\n{key.replace('_', ' ').title()}:\", end='')  # Format the key for display\n",
    "            format_value(value)  # Format the associated value\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")  # Print a separator\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nError: Analysis file '{filename}' not found\\n\")  # Handle file not found error\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"\\nError: Unable to parse JSON from '{filename}'\\n\")  # Handle JSON parsing error\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError displaying JSON details: {str(e)}\\n\")  # Handle any other exceptions\n",
    "\n",
    "# %% cell 4 code\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    for query in test_queries:\n",
    "        print(f\"\\nDisplaying detailed analysis for: {query}\")  # Indicate which query is being processed\n",
    "        display_json_details(query)  # Call the function to display JSON details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
