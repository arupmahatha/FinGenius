{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 20:50:47.022 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/arup/Library/Python/3.12/lib/python/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Imports and Basic Setup\n",
    "import os\n",
    "from typing import Dict, List, Optional, TypedDict, Literal, Union, Annotated\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import json\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AnyMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\n",
    "import sqlite3\n",
    "import re\n",
    "import streamlit as st\n",
    "\n",
    "# Load API keys from environment file\n",
    "load_dotenv('api_key.env')\n",
    "\n",
    "# Initialize memory for state management\n",
    "memory = {}  # Using a simple dictionary for in-memory storage\n",
    "\n",
    "# Part 2: Type Definitions and Base Classes\n",
    "class QueryType(Enum):\n",
    "    DIRECT_SQL = \"direct_sql\"\n",
    "    ANALYSIS = \"analysis\"\n",
    "\n",
    "@dataclass\n",
    "class QueryClassification:\n",
    "    type: QueryType\n",
    "    explanation: str\n",
    "    raw_response: str\n",
    "\n",
    "class AnalysisState(TypedDict):\n",
    "    user_query: str\n",
    "    query_classification: Dict\n",
    "    decomposed_questions: List[str]\n",
    "    sql_results: Dict\n",
    "    analysis: str\n",
    "    final_output: Dict\n",
    "    token_usage: Dict\n",
    "    processing_time: float\n",
    "    agent_states: Dict\n",
    "    raw_responses: Dict\n",
    "    messages: List[AnyMessage]\n",
    "\n",
    "class ConfigError(Exception):\n",
    "    \"\"\"Custom exception for configuration errors\"\"\"\n",
    "    pass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    db_path: str\n",
    "    sqlite_path: str = \"sqlite:///consumption.db\"\n",
    "    model_name: str = \"claude-3-sonnet-20240229\"\n",
    "    human_in_the_loop: bool = False\n",
    "    \n",
    "    @property\n",
    "    def api_key(self) -> str:\n",
    "        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ConfigError(\"ANTHROPIC_API_KEY not found in environment variables\")\n",
    "        if not api_key.startswith('sk-'):\n",
    "            raise ConfigError(\"Invalid API key format\")\n",
    "        return api_key\n",
    "\n",
    "# Part 3: Prompt Templates\n",
    "QUERY_CLASSIFIER_PROMPT = \"\"\"You are a query classifier that determines if a stock market question needs complex analysis or can be answered with a direct SQL query.\n",
    "\n",
    "Example 1:\n",
    "Question: \"Show me the last 5 days of stock prices\"\n",
    "Classification: direct_sql\n",
    "Explanation: This is a straightforward data retrieval request.\n",
    "\n",
    "Example 2:\n",
    "Question: \"What are the emerging trends in trading volume and their impact on price?\"\n",
    "Classification: analysis\n",
    "Explanation: This requires complex analysis of relationships and patterns.\n",
    "\n",
    "Respond in JSON format:\n",
    "{\n",
    "    \"type\": \"direct_sql\" or \"analysis\",\n",
    "    \"explanation\": \"brief explanation of classification\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "SQL_AGENT_PROMPT = \"\"\"You are an expert financial database analyst. Your task is to:\n",
    "1. Analyze stock market queries\n",
    "2. Create appropriate SQL queries\n",
    "3. Provide clear results\n",
    "\n",
    "Example 1:\n",
    "User: \"What's the stock's performance last week?\"\n",
    "Thought: Need to analyze daily price changes and volume for the past week\n",
    "SQL:\n",
    "SELECT \n",
    "    date,\n",
    "    ROUND(open, 2) as open_price,\n",
    "    ROUND(close, 2) as close_price,\n",
    "    ROUND(((close - open) / open * 100), 2) as daily_return,\n",
    "    ROUND(high, 2) as high,\n",
    "    ROUND(low, 2) as low,\n",
    "    volume\n",
    "FROM consumption\n",
    "WHERE date >= date('now', '-7 days')\n",
    "ORDER BY date DESC;\n",
    "\n",
    "Example 2:\n",
    "User: \"Find volatile trading days\"\n",
    "Thought: Looking for days with large price ranges and high volume\n",
    "SQL:\n",
    "WITH metrics AS (\n",
    "    SELECT AVG(volume) as avg_vol,\n",
    "           AVG((high - low) / open * 100) as avg_range\n",
    "    FROM consumption\n",
    ")\n",
    "SELECT \n",
    "    date,\n",
    "    ROUND(open, 2) as open_price,\n",
    "    ROUND(close, 2) as close_price,\n",
    "    ROUND(((high - low) / open * 100), 2) as price_range_pct,\n",
    "    volume,\n",
    "    ROUND(volume / avg_vol, 2) as vol_ratio\n",
    "FROM consumption, metrics\n",
    "WHERE (high - low) / open * 100 > avg_range\n",
    "AND volume > avg_vol\n",
    "ORDER BY price_range_pct DESC\n",
    "LIMIT 5;\n",
    "\n",
    "Your responses should include:\n",
    "1. Thought process\n",
    "2. SQL query\n",
    "3. Result interpretation\"\"\"\n",
    "\n",
    "ANALYST_PROMPT = \"\"\"You are an expert financial analyst. Analyze the provided SQL results and provide insights.\n",
    "\n",
    "Focus on:\n",
    "1. Price trends and patterns\n",
    "2. Volume analysis\n",
    "3. Technical indicators\n",
    "4. Risk assessment\n",
    "5. Notable patterns\n",
    "\n",
    "Example Analysis Structure:\n",
    "1. Key Findings\n",
    "   - Main price trends\n",
    "   - Volume patterns\n",
    "   - Notable events\n",
    "\n",
    "2. Technical Analysis\n",
    "   - Support/resistance levels\n",
    "   - Pattern recognition\n",
    "   - Momentum indicators\n",
    "\n",
    "3. Risk Assessment\n",
    "   - Volatility measures\n",
    "   - Liquidity analysis\n",
    "   - Risk factors\n",
    "\n",
    "4. Recommendations\n",
    "   - Key levels to watch\n",
    "   - Risk considerations\n",
    "   - Potential scenarios\n",
    "\n",
    "Be specific and data-driven in your analysis.\"\"\"\n",
    "\n",
    "# Part 4: Main StockAnalyzer Class\n",
    "class StockAnalyzer:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.db = self._init_database()\n",
    "        self.llm = self._init_llm()\n",
    "        self.sql_agent = self._setup_sql_agent()\n",
    "        self.token_usage = {\"prompt_tokens\": 0, \"completion_tokens\": 0}\n",
    "        self.anthropic_client = Anthropic(api_key=config.api_key)\n",
    "        self.agent_states = {}\n",
    "        self.raw_responses = {}\n",
    "        self.conn = sqlite3.connect('consumption.db')\n",
    "\n",
    "    def _init_database(self) -> SQLDatabase:\n",
    "        return SQLDatabase.from_uri(self.config.sqlite_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_database(csv_path: str) -> None:\n",
    "        try:\n",
    "            # Create SQLite database from CSV\n",
    "            df = pd.read_csv(csv_path)\n",
    "            conn = sqlite3.connect('consumption.db')\n",
    "            df.to_sql('consumption', conn, if_exists='replace', index=False)\n",
    "            conn.close()\n",
    "        except Exception as e:\n",
    "            raise ConfigError(f\"Failed to initialize database: {str(e)}\")\n",
    "\n",
    "    def _init_llm(self) -> ChatAnthropic:\n",
    "        return ChatAnthropic(\n",
    "            model=self.config.model_name,\n",
    "            temperature=0,\n",
    "            api_key=self.config.api_key\n",
    "        )\n",
    "\n",
    "    def _setup_sql_agent(self):\n",
    "        toolkit = SQLDatabaseToolkit(db=self.db, llm=self.llm)\n",
    "        return create_sql_agent(\n",
    "            llm=self.llm,\n",
    "            toolkit=toolkit,\n",
    "            agent_type=\"zero-shot-react-description\",\n",
    "            verbose=True,\n",
    "            prefix=SQL_AGENT_PROMPT\n",
    "        )\n",
    "\n",
    "    def _get_human_input(self, prompt: str, default_value=None) -> str:\n",
    "        if not self.config.human_in_the_loop:\n",
    "            return default_value\n",
    "        response = input(f\"\\n{prompt}\\nPress Enter to accept default or input your modification: \")\n",
    "        return response if response.strip() else default_value\n",
    "\n",
    "    def analyze(self, query: str) -> Dict:\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # Reset storages for new analysis\n",
    "            self.token_usage = {\"prompt_tokens\": 0, \"completion_tokens\": 0}\n",
    "            self.agent_states = {}\n",
    "            self.raw_responses = {}\n",
    "            \n",
    "            # First, classify the query\n",
    "            classification = self._classify_query(query)\n",
    "            \n",
    "            # Human review of classification if enabled\n",
    "            if self.config.human_in_the_loop:\n",
    "                human_classification = self._get_human_input(\n",
    "                    f\"Query Classification: {classification.type.value}\\nExplanation: {classification.explanation}\\n\"\n",
    "                    \"Enter 'direct_sql' or 'analysis' to modify, or press Enter to accept: \",\n",
    "                    classification.type.value\n",
    "                )\n",
    "                if human_classification in ['direct_sql', 'analysis']:\n",
    "                    classification.type = QueryType(human_classification)\n",
    "            \n",
    "            # For direct SQL queries, use simplified processing\n",
    "            if classification.type == QueryType.DIRECT_SQL:\n",
    "                return self._direct_sql_query(query)\n",
    "            \n",
    "            # For analysis queries, use decomposition approach\n",
    "            decomposed_questions = self._decompose_question(query)\n",
    "            \n",
    "            # Human review of decomposed questions if enabled\n",
    "            if self.config.human_in_the_loop:\n",
    "                print(\"\\nDecomposed Questions:\")\n",
    "                for i, q in enumerate(decomposed_questions):\n",
    "                    modified_q = self._get_human_input(f\"Question {i+1}: {q}\", q)\n",
    "                    decomposed_questions[i] = modified_q\n",
    "            \n",
    "            sql_results = self._run_sql_analysis(decomposed_questions)\n",
    "            \n",
    "            # Human review of SQL results if enabled\n",
    "            if self.config.human_in_the_loop:\n",
    "                print(\"\\nSQL Results Review:\")\n",
    "                for key, data in sql_results.items():\n",
    "                    if 'sql' in data:\n",
    "                        modified_sql = self._get_human_input(f\"Review SQL for {key}:\\n{data['sql']}\", data['sql'])\n",
    "                        if modified_sql != data['sql']:\n",
    "                            try:\n",
    "                                df = pd.read_sql_query(modified_sql, self.conn)\n",
    "                                data['sql'] = modified_sql\n",
    "                                data['result'] = df.to_dict('records')\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error with modified SQL: {str(e)}\")\n",
    "            \n",
    "            analysis = self._analyze_results(query, sql_results)\n",
    "            \n",
    "            # Human review of analysis if enabled\n",
    "            if self.config.human_in_the_loop:\n",
    "                modified_analysis = self._get_human_input(\n",
    "                    f\"Review Analysis:\\n{analysis}\\nEnter modifications or press Enter to accept: \",\n",
    "                    analysis\n",
    "                )\n",
    "                if modified_analysis != analysis:\n",
    "                    analysis = modified_analysis\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            final_output = {\n",
    "                \"query_type\": \"analysis\",\n",
    "                \"user_query\": query,\n",
    "                \"query_classification\": {\n",
    "                    \"type\": classification.type.value,\n",
    "                    \"explanation\": classification.explanation,\n",
    "                    \"raw_response\": classification.raw_response\n",
    "                },\n",
    "                \"sub_questions\": decomposed_questions,\n",
    "                \"sql_analysis\": sql_results,\n",
    "                \"expert_analysis\": analysis,\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "                \"token_usage\": self.token_usage,\n",
    "                \"processing_time\": processing_time,\n",
    "                \"agent_states\": self.agent_states,\n",
    "                \"raw_responses\": self.raw_responses\n",
    "            }\n",
    "            \n",
    "            # Save output to file\n",
    "            filename = f\"{query[:50].replace(' ', '_').lower()}_analysis.json\"\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(final_output, f, indent=2)\n",
    "                \n",
    "            return final_output\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"query\": query}\n",
    "        finally:\n",
    "            self.conn.close()\n",
    "\n",
    "    def _classify_query(self, query: str) -> QueryClassification:\n",
    "        try:\n",
    "            response = self.llm.invoke([\n",
    "                SystemMessage(content=QUERY_CLASSIFIER_PROMPT),\n",
    "                HumanMessage(content=f\"Classify this question: {query}\")\n",
    "            ])\n",
    "            \n",
    "            self._update_token_usage(response)\n",
    "            classification = json.loads(response.content)\n",
    "            \n",
    "            self.raw_responses['classification'] = response.content\n",
    "            \n",
    "            return QueryClassification(\n",
    "                type=QueryType(classification['type']),\n",
    "                explanation=classification['explanation'],\n",
    "                raw_response=response.content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return QueryClassification(\n",
    "                type=QueryType.ANALYSIS,\n",
    "                explanation=\"Classification failed, defaulting to analysis\",\n",
    "                raw_response=str(e)\n",
    "            )\n",
    "\n",
    "    def _direct_sql_query(self, query: str) -> Dict:\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            result = self.sql_agent.invoke({\"input\": query})\n",
    "            self._update_token_usage(result)\n",
    "            \n",
    "            self.agent_states['direct_sql'] = result\n",
    "            \n",
    "            thought = self._extract_thought(result['output'])\n",
    "            sql = self._extract_sql(result['output'])\n",
    "            \n",
    "            # Human review of SQL if enabled\n",
    "            if self.config.human_in_the_loop:\n",
    "                modified_sql = self._get_human_input(\n",
    "                    f\"Review SQL Query:\\n{sql}\\nEnter modifications or press Enter to accept: \",\n",
    "                    sql\n",
    "                )\n",
    "                if modified_sql != sql:\n",
    "                    sql = modified_sql\n",
    "            \n",
    "            try:\n",
    "                if not sql:\n",
    "                    sql_match = re.search(r'SELECT.*?;', result['output'], re.DOTALL | re.IGNORECASE)\n",
    "                    if sql_match:\n",
    "                        sql = sql_match.group(0)\n",
    "                    else:\n",
    "                        raise ValueError(\"Could not extract SQL query from agent output\")\n",
    "                \n",
    "                sql = sql.split(';')[0] + ';'\n",
    "                \n",
    "                df = pd.read_sql_query(sql, self.conn)\n",
    "                formatted_results = df.to_dict('records')\n",
    "            except Exception as e:\n",
    "                formatted_results = f\"Error executing SQL: {str(e)}\"\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            output_data = {\n",
    "                \"query_type\": \"direct_sql\",\n",
    "                \"user_query\": query,\n",
    "                \"thought_process\": thought if thought else \"No thought process provided\",\n",
    "                \"sql_query\": sql,\n",
    "                \"results\": formatted_results,\n",
    "                \"raw_agent_output\": result['output'],\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "                \"token_usage\": self.token_usage,\n",
    "                \"processing_time\": processing_time,\n",
    "                \"agent_state\": result\n",
    "            }\n",
    "            \n",
    "            filename = f\"{query[:50].replace(' ', '_').lower()}_analysis.json\"\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(output_data, f, indent=2)\n",
    "                \n",
    "            return output_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"query\": query}\n",
    "\n",
    "    def _decompose_question(self, query: str) -> List[str]:\n",
    "        response = self.llm.invoke([\n",
    "            SystemMessage(content=\"Break down this stock analysis question into specific sub-questions that can be answered with SQL queries:\"),\n",
    "            HumanMessage(content=query)\n",
    "        ])\n",
    "        \n",
    "        self._update_token_usage(response)\n",
    "        self.raw_responses['decomposition'] = response.content\n",
    "        \n",
    "        questions = [\n",
    "            q.strip().split(\". \", 1)[1] if \". \" in q else q.strip()\n",
    "            for q in response.content.split(\"\\n\")\n",
    "            if q.strip() and q[0].isdigit()\n",
    "        ]\n",
    "        \n",
    "        return questions\n",
    "\n",
    "    def _run_sql_analysis(self, questions: List[str]) -> Dict:\n",
    "        results = {}\n",
    "        agent_states = {}\n",
    "        \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            try:\n",
    "                result = self.sql_agent.invoke({\"input\": question})\n",
    "                self._update_token_usage(result)\n",
    "                \n",
    "                agent_states[f\"question_{i}\"] = result\n",
    "                \n",
    "                thought = self._extract_thought(result['output'])\n",
    "                sql = self._extract_sql(result['output'])\n",
    "                \n",
    "                try:\n",
    "                    sql = sql.split(';')[0] + ';'\n",
    "                    df = pd.read_sql_query(sql, self.conn)\n",
    "                    parsed_result = df.to_dict('records')\n",
    "                except Exception as e:\n",
    "                    parsed_result = f\"Error executing SQL: {str(e)}\"\n",
    "                \n",
    "                results[f\"question_{i}\"] = {\n",
    "                    \"question\": question,\n",
    "                    \"thought\": thought if thought else \"No thought process provided\",\n",
    "                    \"sql\": sql if sql else \"No SQL query provided\",\n",
    "                    \"result\": parsed_result,\n",
    "                    \"raw_output\": result['output']\n",
    "                }\n",
    "                    \n",
    "            except Exception as e:\n",
    "                results[f\"question_{i}\"] = {\n",
    "                    \"error\": str(e),\n",
    "                    \"question\": question\n",
    "                }\n",
    "        \n",
    "        self.agent_states['sql_analysis'] = agent_states\n",
    "        return results\n",
    "\n",
    "    def _analyze_results(self, query: str, sql_results: Dict) -> str:\n",
    "        results_context = json.dumps(sql_results, indent=2)\n",
    "        response = self.llm.invoke([\n",
    "            SystemMessage(content=ANALYST_PROMPT),\n",
    "            HumanMessage(content=f\"\"\"\n",
    "            Original Question: {query}\n",
    "            \n",
    "            Analysis Results:\n",
    "            {results_context}\n",
    "            \n",
    "            Provide a comprehensive analysis.\"\"\")\n",
    "        ])\n",
    "        \n",
    "        self._update_token_usage(response)\n",
    "        self.raw_responses['analysis'] = response.content\n",
    "        \n",
    "        return response.content\n",
    "\n",
    "    def _update_token_usage(self, response):\n",
    "        try:\n",
    "            if hasattr(response, '_raw_response') and 'usage' in response._raw_response:\n",
    "                usage = response._raw_response['usage']\n",
    "                self.token_usage[\"prompt_tokens\"] += usage.get('input_tokens', 0)\n",
    "                self.token_usage[\"completion_tokens\"] += usage.get('output_tokens', 0)\n",
    "            elif isinstance(response, dict) and 'usage' in response:\n",
    "                usage = response['usage']\n",
    "                self.token_usage[\"prompt_tokens\"] += usage.get('input_tokens', 0)\n",
    "                self.token_usage[\"completion_tokens\"] += usage.get('output_tokens', 0)\n",
    "            elif hasattr(response, 'usage'):\n",
    "                usage = response.usage\n",
    "                self.token_usage[\"prompt_tokens\"] += usage.input_tokens if hasattr(usage, 'input_tokens') else 0\n",
    "                self.token_usage[\"completion_tokens\"] += usage.output_tokens if hasattr(usage, 'output_tokens') else 0\n",
    "            else:\n",
    "                message = response.content if hasattr(response, 'content') else str(response)\n",
    "                result = self.anthropic_client.messages.create(\n",
    "                    model=self.config.model_name,\n",
    "                    messages=[{\"role\": \"user\", \"content\": message}],\n",
    "                    max_tokens=1\n",
    "                )\n",
    "                if hasattr(result, 'usage'):\n",
    "                    self.token_usage[\"prompt_tokens\"] += result.usage.input_tokens\n",
    "                    self.token_usage[\"completion_tokens\"] += result.usage.output_tokens\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating token usage: {str(e)}\")\n",
    "\n",
    "    def _extract_thought(self, text: str) -> str:\n",
    "        if \"Thought:\" in text:\n",
    "            return text.split(\"Thought:\")[1].split(\"SQL\")[0].strip()\n",
    "        return \"\"\n",
    "\n",
    "    def _extract_sql(self, text: str) -> str:\n",
    "        if \"SQL:\" in text:\n",
    "            sql_part = text.split(\"SQL:\")[1]\n",
    "            if \"SQLResult:\" in sql_part:\n",
    "                return sql_part.split(\"SQLResult:\")[0].strip()\n",
    "            if \"Final Answer:\" in sql_part:\n",
    "                return sql_part.split(\"Final Answer:\")[0].strip()\n",
    "            return sql_part.strip()\n",
    "        return \"\"\n",
    "\n",
    "def format_output(results: Dict) -> str:\n",
    "    output = []\n",
    "    output.append(\"=== Stock Analysis Results ===\")\n",
    "    output.append(f\"\\nQuery: {results.get('user_query', 'N/A')}\")\n",
    "    \n",
    "    output.append(f\"\\nProcessing Time: {results.get('processing_time', 0):.2f} seconds\")\n",
    "    token_usage = results.get('token_usage', {})\n",
    "    output.append(f\"Token Usage:\")\n",
    "    output.append(f\"  Prompt Tokens: {token_usage.get('prompt_tokens', 0)}\")\n",
    "    output.append(f\"  Completion Tokens: {token_usage.get('completion_tokens', 0)}\")\n",
    "    output.append(f\"  Total Tokens: {token_usage.get('prompt_tokens', 0) + token_usage.get('completion_tokens', 0)}\")\n",
    "    \n",
    "    if \"error\" in results:\n",
    "        output.append(f\"\\nError: {results['error']}\")\n",
    "        return \"\\n\".join(output)\n",
    "    \n",
    "    if results.get('query_type') == 'direct_sql':\n",
    "        output.append(f\"\\nThought Process: {results.get('thought_process', 'N/A')}\")\n",
    "        output.append(f\"\\nSQL Query: {results.get('sql_query', 'N/A')}\")\n",
    "        output.append(\"\\nResults:\")\n",
    "        if isinstance(results.get('results'), list):\n",
    "            df = pd.DataFrame(results['results'])\n",
    "            output.append(str(df))\n",
    "        else:\n",
    "            output.append(str(results.get('results', 'No results available')))\n",
    "    else:\n",
    "        output.append(\"\\nSub-Questions:\")\n",
    "        for i, q in enumerate(results.get('sub_questions', []), 1):\n",
    "            output.append(f\"{i}. {q}\")\n",
    "        \n",
    "        output.append(\"\\nSQL Analysis:\")\n",
    "        for key, data in results.get('sql_analysis', {}).items():\n",
    "            output.append(f\"\\nQuestion: {data.get('question', 'N/A')}\")\n",
    "            if 'error' not in data:\n",
    "                output.append(f\"Thought Process: {data.get('thought', 'N/A')}\")\n",
    "                output.append(f\"SQL Query: {data.get('sql', 'N/A')}\")\n",
    "                try:\n",
    "                    if isinstance(data.get('result'), (list, dict)):\n",
    "                        df = pd.DataFrame(data['result'])\n",
    "                        output.append(str(df))\n",
    "                    else:\n",
    "                        output.append(f\"Results: {data.get('result', 'No results available')}\")\n",
    "                except:\n",
    "                    output.append(f\"Results: {data.get('result', 'No results available')}\")\n",
    "            else:\n",
    "                output.append(f\"Error: {data['error']}\")\n",
    "        \n",
    "        output.append(\"\\nExpert Analysis:\")\n",
    "        output.append(results.get('expert_analysis', 'No analysis available'))\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "def analyze_stock_query(query: str, csv_file: str, human_in_the_loop: bool = False) -> str:\n",
    "    try:\n",
    "        config = Config(db_path=csv_file, human_in_the_loop=human_in_the_loop)\n",
    "        StockAnalyzer.initialize_database(csv_file)\n",
    "        analyzer = StockAnalyzer(config)\n",
    "        results = analyzer.analyze(query)\n",
    "        \n",
    "        if results and \"error\" not in results:\n",
    "            formatted_output = format_output(results)\n",
    "            filename = f\"{query[:50].replace(' ', '_').lower()}_analysis.json\"\n",
    "            return formatted_output + f\"\\n\\nDetailed results saved to {filename}\"\n",
    "        else:\n",
    "            return f\"Error: {results.get('error', 'Unknown error occurred')}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error during analysis: {str(e)}\"\n",
    "\n",
    "# Save this file as stock_analyzer.py\n",
    "if __name__ == \"__main__\":\n",
    "    st.set_page_config(page_title=\"Stock Market Data Analyzer\", layout=\"wide\")\n",
    "    \n",
    "    st.title(\"Stock Market Data Analyzer\")\n",
    "    st.write(\"Upload your stock market data CSV file and analyze it using natural language queries.\")\n",
    "\n",
    "    # File upload\n",
    "    uploaded_file = st.file_uploader(\"Choose a CSV file\", type=\"csv\")\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        # Save uploaded file temporarily\n",
    "        with open(\"temp.csv\", \"wb\") as f:\n",
    "            f.write(uploaded_file.getvalue())\n",
    "        \n",
    "        # Display sample data\n",
    "        df = pd.read_csv(\"temp.csv\")\n",
    "        st.subheader(\"Preview of uploaded data\")\n",
    "        st.dataframe(df.head())\n",
    "        \n",
    "        # Number of questions input\n",
    "        num_questions = st.number_input(\"How many questions would you like to ask?\", min_value=1, max_value=10, value=1)\n",
    "        \n",
    "        # Create text input fields for each question\n",
    "        questions = []\n",
    "        for i in range(int(num_questions)):\n",
    "            question = st.text_input(f\"Question {i+1}\", key=f\"question_{i}\")\n",
    "            questions.append(question)\n",
    "        \n",
    "        # Analysis button\n",
    "        if st.button(\"Analyze Questions\"):\n",
    "            if all(questions):\n",
    "                progress_bar = st.progress(0)\n",
    "                for i, query in enumerate(questions, 1):\n",
    "                    st.subheader(f\"Analysis for Question {i}: {query}\")\n",
    "                    progress_text = st.empty()\n",
    "                    progress_text.text(\"Analyzing...\")\n",
    "                    \n",
    "                    try:\n",
    "                        with st.spinner(f\"Analyzing question {i}...\"):\n",
    "                            result = analyze_stock_query(query, \"temp.csv\")\n",
    "                            st.write(result)\n",
    "                        progress_bar.progress(i/len(questions))\n",
    "                    except Exception as e:\n",
    "                        st.error(f\"Error analyzing question {i}: {str(e)}\")\n",
    "                    \n",
    "                    progress_text.empty()\n",
    "            else:\n",
    "                st.warning(\"Please fill in all question fields\")\n",
    "\n",
    "        # Cleanup\n",
    "        if os.path.exists(\"temp.csv\"):\n",
    "            os.remove(\"temp.csv\")\n",
    "    else:\n",
    "        st.info(\"Please upload a CSV file to begin analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamlit run /Users/arup/Library/Python/3.12/lib/python/site-packages/ipykernel_launcher.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
