{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected 'except' or 'finally' block (3801869233.py, line 290)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 290\u001b[0;36m\u001b[0m\n\u001b[0;31m    if __name__ == \"__main__\":\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected 'except' or 'finally' block\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, List, Optional, TypedDict\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import json\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('api_key.env')\n",
    "\n",
    "class AnalysisState(TypedDict):\n",
    "    \"\"\"State for the analysis workflow\"\"\"\n",
    "    user_query: str\n",
    "    decomposed_questions: List[str]\n",
    "    sql_results: Dict\n",
    "    analysis: str\n",
    "    final_output: Dict\n",
    "\n",
    "class ConfigError(Exception):\n",
    "    \"\"\"Custom exception for configuration errors\"\"\"\n",
    "    pass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    db_path: str = \"apple_last_year_data.csv\"\n",
    "    sqlite_path: str = \"sqlite:///consumption.db\"\n",
    "    model_name: str = \"claude-3-sonnet-20240229\"\n",
    "    \n",
    "    @property\n",
    "    def api_key(self) -> str:\n",
    "        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ConfigError(\"ANTHROPIC_API_KEY not found in api_key.env file\")\n",
    "        return api_key\n",
    "\n",
    "SQL_AGENT_PROMPT = \"\"\"You are an expert financial database analyst. Analyze questions and create SQL queries for stock data analysis.\n",
    "\n",
    "Available Columns: date, open, high, low, close, volume\n",
    "\n",
    "Example 1:\n",
    "User: \"Show me the performance metrics for last week\"\n",
    "Thought: Need to calculate key metrics including price changes, trading ranges, and volume patterns for the past week\n",
    "SQL:\n",
    "SELECT \n",
    "    date,\n",
    "    ROUND(open, 2) as open_price,\n",
    "    ROUND(close, 2) as close_price,\n",
    "    ROUND(((close - open) / open * 100), 2) as daily_change_percent,\n",
    "    ROUND(high, 2) as high_price,\n",
    "    ROUND(low, 2) as low_price,\n",
    "    volume\n",
    "FROM consumption\n",
    "WHERE date >= date('now', '-7 days')\n",
    "ORDER BY date DESC;\n",
    "\n",
    "Example 2:\n",
    "User: \"Identify days with high volatility\"\n",
    "Thought: Looking for days with large price ranges and above-average volume\n",
    "SQL:\n",
    "WITH avg_metrics AS (\n",
    "    SELECT \n",
    "        AVG(volume) as avg_volume,\n",
    "        AVG((high - low) / open * 100) as avg_range_percent\n",
    "    FROM consumption\n",
    ")\n",
    "SELECT \n",
    "    date,\n",
    "    ROUND(open, 2) as open_price,\n",
    "    ROUND(close, 2) as close_price,\n",
    "    ROUND(((high - low) / open * 100), 2) as price_range_percent,\n",
    "    volume,\n",
    "    ROUND(volume / avg_volume, 2) as volume_ratio\n",
    "FROM consumption, avg_metrics\n",
    "WHERE (high - low) / open * 100 > avg_range_percent\n",
    "    AND volume > avg_volume\n",
    "ORDER BY price_range_percent DESC;\n",
    "\n",
    "Format your response as:\n",
    "1. Thought: Your analysis approach\n",
    "2. SQL: Your query\n",
    "3. Explanation: Interpret the results\n",
    "\"\"\"\n",
    "\n",
    "ANALYST_PROMPT = \"\"\"You are an expert financial analyst. Given SQL query results, provide comprehensive market analysis.\n",
    "\n",
    "Example 1:\n",
    "Data: Price trend data with volumes\n",
    "Analysis:\n",
    "1. Price Trend: Identify direction and strength\n",
    "2. Volume Analysis: Look for confirmation/divergence\n",
    "3. Key Levels: Support/resistance points\n",
    "4. Pattern Recognition: Technical patterns\n",
    "5. Risk Assessment: Volatility and liquidity\n",
    "\n",
    "Example 2:\n",
    "Data: Volatility metrics\n",
    "Analysis:\n",
    "1. Volatility Trends: Changes in price ranges\n",
    "2. Volume Impact: Relationship with price moves\n",
    "3. Market Sentiment: What patterns suggest\n",
    "4. Risk Levels: Current vs historical\n",
    "5. Trading Implications: What data suggests\n",
    "\n",
    "Provide:\n",
    "1. Key Findings: Main observations\n",
    "2. Technical Analysis: Pattern analysis\n",
    "3. Risk Assessment: Current risk levels\n",
    "4. Action Items: Suggested next steps\n",
    "\"\"\"\n",
    "\n",
    "class IntegratedAnalyzer:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.db = self._init_database()\n",
    "        self.llm = self._init_llm()\n",
    "        self.sql_agent = self._setup_sql_agent()\n",
    "        self.workflow = self._create_workflow()\n",
    "\n",
    "    def _init_database(self) -> SQLDatabase:\n",
    "        df = pd.read_csv(self.config.db_path)\n",
    "        df.to_sql('consumption', 'sqlite:///consumption.db', index=False, if_exists='replace')\n",
    "        return SQLDatabase.from_uri(self.config.sqlite_path)\n",
    "\n",
    "    def _init_llm(self) -> ChatAnthropic:\n",
    "        return ChatAnthropic(\n",
    "            model=self.config.model_name,\n",
    "            temperature=0,\n",
    "            api_key=self.config.api_key\n",
    "        )\n",
    "\n",
    "    def _setup_sql_agent(self):\n",
    "        toolkit = SQLDatabaseToolkit(db=self.db, llm=self.llm)\n",
    "        return create_sql_agent(\n",
    "            llm=self.llm,\n",
    "            toolkit=toolkit,\n",
    "            agent_type=\"zero-shot-react-description\",\n",
    "            verbose=True,\n",
    "            prefix=SQL_AGENT_PROMPT\n",
    "        )\n",
    "\n",
    "    def _decompose_question(self, state: AnalysisState) -> AnalysisState:\n",
    "        prompt = f\"\"\"Break down this analysis question into specific sub-questions:\n",
    "        Question: {state['user_query']}\n",
    "        Return only numbered questions.\"\"\"\n",
    "        \n",
    "        response = self.llm.invoke([SystemMessage(content=prompt)])\n",
    "        questions = [q.strip().split(\". \", 1)[1] for q in response.content.split(\"\\n\") \n",
    "                    if q.strip() and q[0].isdigit()]\n",
    "        \n",
    "        return {**state, \"decomposed_questions\": questions}\n",
    "\n",
    "    def _run_sql_analysis(self, state: AnalysisState) -> AnalysisState:\n",
    "        results = {}\n",
    "        for i, question in enumerate(state[\"decomposed_questions\"]):\n",
    "            result = self.sql_agent.invoke({\"input\": question})\n",
    "            results[f\"question_{i+1}\"] = {\n",
    "                \"question\": question,\n",
    "                \"thought\": self._extract_thought(result['output']),\n",
    "                \"sql\": self._extract_sql(result['output']),\n",
    "                \"result\": self._extract_result(result['output'])\n",
    "            }\n",
    "        \n",
    "        return {**state, \"sql_results\": results}\n",
    "\n",
    "    def _analyze_results(self, state: AnalysisState) -> AnalysisState:\n",
    "        results_text = json.dumps(state[\"sql_results\"], indent=2)\n",
    "        analysis = self.llm.invoke([\n",
    "            SystemMessage(content=ANALYST_PROMPT),\n",
    "            HumanMessage(content=f\"Analyze these results for query '{state['user_query']}':\\n{results_text}\")\n",
    "        ])\n",
    "        \n",
    "        return {**state, \"analysis\": analysis.content}\n",
    "\n",
    "    def _format_output(self, state: AnalysisState) -> AnalysisState:\n",
    "        final_output = {\n",
    "            \"query\": state[\"user_query\"],\n",
    "            \"decomposed_questions\": state[\"decomposed_questions\"],\n",
    "            \"sql_analysis\": state[\"sql_results\"],\n",
    "            \"expert_analysis\": state[\"analysis\"],\n",
    "            \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return {**state, \"final_output\": final_output}\n",
    "\n",
    "    def _extract_thought(self, text: str) -> str:\n",
    "        if \"Thought:\" in text:\n",
    "            return text.split(\"Thought:\")[1].split(\"SQL\")[0].strip()\n",
    "        return \"\"\n",
    "\n",
    "    def _extract_sql(self, text: str) -> str:\n",
    "        if \"SQL:\" in text:\n",
    "            return text.split(\"SQL:\")[1].split(\"Result\")[0].strip()\n",
    "        return \"\"\n",
    "\n",
    "    def _extract_result(self, text: str) -> str:\n",
    "        if \"SQLResult:\" in text:\n",
    "            return text.split(\"SQLResult:\")[1].strip()\n",
    "        return \"\"\n",
    "\n",
    "    def _create_workflow(self) -> StateGraph:\n",
    "        workflow = StateGraph(AnalysisState)\n",
    "        \n",
    "        workflow.add_node(\"decompose\", self._decompose_question)\n",
    "        workflow.add_node(\"sql_analysis\", self._run_sql_analysis)\n",
    "        workflow.add_node(\"analyze\", self._analyze_results)\n",
    "        workflow.add_node(\"format\", self._format_output)\n",
    "        \n",
    "        workflow.add_edge(START, \"decompose\")\n",
    "        workflow.add_edge(\"decompose\", \"sql_analysis\")\n",
    "        workflow.add_edge(\"sql_analysis\", \"analyze\")\n",
    "        workflow.add_edge(\"analyze\", \"format\")\n",
    "        workflow.add_edge(\"format\", END)\n",
    "        \n",
    "        return workflow.compile()\n",
    "\n",
    "    def analyze(self, query: str) -> Dict:\n",
    "        try:\n",
    "            final_state = None\n",
    "            for state in self.workflow.stream({\n",
    "                \"user_query\": query,\n",
    "                \"decomposed_questions\": [],\n",
    "                \"sql_results\": {},\n",
    "                \"analysis\": \"\",\n",
    "                \"final_output\": {}\n",
    "            }):\n",
    "                final_state = state\n",
    "            \n",
    "            return final_state[\"final_output\"]\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"query\": query}\n",
    "\n",
    "def format_output(results: Dict) -> None:\n",
    "    print(\"\\n=== Stock Analysis Results ===\")\n",
    "    print(f\"\\nQuery: {results['query']}\")\n",
    "    \n",
    "    print(\"\\nDecomposed Questions:\")\n",
    "    for i, q in enumerate(results['decomposed_questions'], 1):\n",
    "        print(f\"{i}. {q}\")\n",
    "    \n",
    "    print(\"\\nSQL Analysis:\")\n",
    "    for key, data in results['sql_analysis'].items():\n",
    "        print(f\"\\nQuestion: {data['question']}\")\n",
    "        print(f\"Thought Process: {data['thought']}\")\n",
    "        print(f\"SQL Query: {data['sql']}\")\n",
    "        try:\n",
    "            result_data = json.loads(data['result'])\n",
    "            df = pd.DataFrame(result_data)\n",
    "            print(\"\\nResults:\")\n",
    "            print(df.to_string(index=False))\n",
    "        except:\n",
    "            print(f\"Results: {data['result']}\")\n",
    "    \n",
    "    print(\"\\nExpert Analysis:\")\n",
    "    print(results['expert_analysis'])\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        config = Config()\n",
    "        analyzer = IntegratedAnalyzer(config)\n",
    "        \n",
    "        queries = [\n",
    "            \"Show me the stock's trend over the last week with daily price ranges\",\n",
    "            \"Find days where volume was significantly above average\",\n",
    "            \"Analyze the relationship between daily price ranges and volume\"\n",
    "        ]\n",
    "        \n",
    "        for query in queries:\n",
    "            print(f\"\\nProcessing: {query}\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            results = analyzer.analyze(query)\n",
    "            \n",
    "            if \"error\" not in results:\n",
    "                format_output(results)\n",
    "                \n",
    "                # Save to JSON\n",
    "                filename = f\"analysis_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "                with open(filename, 'w') as f:\n",
    "                    json.dump(results, f, indent=2)\n",
    "                print(f\"\\nDetailed results saved to {filename}\")\n",
    "            else:\n",
    "                print(f\"Error: {results['error']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
